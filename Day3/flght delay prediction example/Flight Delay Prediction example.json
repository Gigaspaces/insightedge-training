{"paragraphs":[{"text":"%md\n> If you see 'Interpreter binding' above, just hit `Save` without deselecting any interpreters.\n\n## Welcome to Flight Delay prediction example.\n\n##### This example shows the fokkowing:\n\n* 1.Import dependencies - including space data class\n* 2.Read csv file containing actual flight details with delays into the space calss. \n* 3.Possible to Enrich the delay records with weather data of the same date at flight origin and destination and if its holiday etc. \n* 4.Save data to the grid\n* 5.Train machine learning  algo with grid data so it will be able to predict if there will be delay(todo test + improve the model ) \n* 6.Save the model to the grid \n* 7.Load the model from the grid and predict flight delays for new data coming from kaka this will usually be done in diffrent job(possible to add  weather forecast for related date and loacationd)\n* 8.Inform the grid on predicted delays\n* 9.Grid will use delay objects to trigger logic (event container in space)\n\n##### This is a live tutorial, you can run the code yourself. _(click `Run` button in each paragraph from top to bottom)_","user":"anonymous","dateUpdated":"2018-12-09T11:26:20+0200","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<blockquote>\n  <p>If you see &lsquo;Interpreter binding&rsquo; above, just hit <code>Save</code> without deselecting any interpreters.</p>\n</blockquote>\n<h2>Welcome to Flights example.</h2>\n<h5>This example shows some features of InsightEdge:</h5>\n<ul>\n  <li>1.Import dependencies - including data class</li>\n  <li>2.Read csv file containing actual flight details with delays into the space calss.</li>\n  <li>3.Possible to Enrich the delay records with weather data of the same date at flight origin and destination and if its holiday etc.</li>\n  <li>4.Save data to the grid</li>\n  <li>5.Train machine learning algo with grid data so it will be able to predict if there will be delay(todo test + improve the model )</li>\n  <li>6.Save the model to the grid</li>\n  <li>7.Load the model from the grid and predict flight delays for new data coming from kaka (possible to add weather forecast for related date and loacationd)</li>\n  <li>8.Inform the grid on predicted delays</li>\n  <li>9.Grid will use delay objects to trigger logic (event container in space)</li>\n</ul>\n<h5>This is a live tutorial, you can run the code yourself. <em>(click <code>Run</code> button in each paragraph from top to bottom)</em></h5>\n</div>"}]},"apps":[],"jobName":"paragraph_1543831941691_-928495727","id":"20170620-093024_406952967","dateCreated":"2018-12-03T12:12:21+0200","dateStarted":"2018-12-06T14:55:16+0200","dateFinished":"2018-12-06T14:55:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:13387"},{"title":"Import dependencies","text":"%dep\n\nz.reset()\n// Space classes dependecies\nz.load(\"C:/Users/ester/Desktop/FlightDelayDemo/target/ieml-1.0.0-SNAPSHOT.jar\")\n\n//kafka dependencies\nz.load(\"org.apache.spark:spark-streaming_2.10:jar:1.6.2\")\nz.load(\"org.apache.spark:spark-streaming-kafka_2.10:jar:1.6.2\")\nz.load(\"org.apache.kafka:kafka_2.10:jar:0.10.0.1\")\nz.load(\"org.apache.spark:spark-sql-kafka-0-10_2.10\")\nz.load(\"com.yammer.metrics:metrics-core:jar:2.2.0\")\nz.load(\"com.101tec:zkclient:jar:0.9\")","user":"anonymous","dateUpdated":"2018-12-17T12:05:19+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@6ffcebd6\n"}]},"apps":[],"jobName":"paragraph_1543831941722_1893941705","id":"20180103-104153_1795376572","dateCreated":"2018-12-03T12:12:21+0200","dateStarted":"2018-12-09T11:40:40+0200","dateFinished":"2018-12-09T11:40:59+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13388"},{"title":"Saving Flight delay RDD to the Grid after reading from csv","text":"%spark\n\nimport common.DelayRec\nimport org.insightedge.spark.implicits.all._\n\n//Reading and filtering the data\nval data = sc.textFile(\"C:/Users/ester/Desktop/gs/12.3/2007.csv\").map(line => line.split(\",\")).filter(rec => rec(4).toString() != \"NA\").filter(rec => rec(0) !=\"Year\").filter(rec => rec(16).toString() == \"ORD\").filter(rec => rec(21).toString() != \"1\").map(rec => new DelayRec( rec(9).toString(),rec(0).toInt,rec(1).toInt,rec(2).toInt,rec(3).toInt,rec(5).toInt,rec(15).toInt,rec(16).toString(),rec(18).toDouble))\n//Filter header\n//filter canceled flights\n//filter origin to have less records in the example\n\n   \n//see how the data looks like    \ndata.take(5).map(x => x.toString() ).foreach(println)\n\n//save the data to the grid (routing is flight number, id is flight+date)\ndata.saveToGrid()\n    \n\n","user":"anonymous","dateUpdated":"2018-12-13T13:35:38+0200","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import common.DelayRec\nimport org.insightedge.spark.implicits.all._\ndata: org.apache.spark.rdd.RDD[common.DelayRec] = MapPartitionsRDD[16] at map at <console>:39\nDelayRec{id='1202_2007:1:25', flightNumber='1202', year=2007, month=1, dayOfMonth=25, dayOfWeek=4, crsDepTime=1100, depDelay=-8, origin='ORD', distance=719.0}\r\nDelayRec{id='2836_2007:1:28', flightNumber='2836', year=2007, month=1, dayOfMonth=28, dayOfWeek=7, crsDepTime=1500, depDelay=41, origin='ORD', distance=925.0}\r\nDelayRec{id='2701_2007:1:29', flightNumber='2701', year=2007, month=1, dayOfMonth=29, dayOfWeek=1, crsDepTime=2000, depDelay=45, origin='ORD', distance=316.0}\r\nDelayRec{id='1206_2007:1:17', flightNumber='1206', year=2007, month=1, dayOfMonth=17, dayOfWeek=3, crsDepTime=1900, depDelay=-9, origin='ORD', distance=719.0}\r\nDelayRec{id='2023_2007:1:12', flightNumber='2023', year=2007, month=1, dayOfMonth=12, dayOfWeek=5, crsDepTime=1745, depDelay=180, origin='ORD', distance=316.0}\r\n"}]},"apps":[],"jobName":"paragraph_1543831941727_-18366314","id":"20170620-093337_2060899833","dateCreated":"2018-12-03T12:12:21+0200","dateStarted":"2018-12-09T11:44:58+0200","dateFinished":"2018-12-09T11:45:28+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13389"},{"title":"Loading RDD from the grid ","text":"%spark\nval delayRecRdd = sc.gridRdd[DelayRec]()\nval count = delayRecRdd.count()\n","user":"anonymous","dateUpdated":"2018-12-03T12:12:21+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndelayRecRdd: org.insightedge.spark.rdd.InsightEdgeRDD[common.DelayRec] = InsightEdgeRDD[16] at RDD at InsightEdgeAbstractRDD.scala:36\n\ncount: Long = 343480\n"}]},"apps":[],"jobName":"paragraph_1543831941730_740064036","id":"20170620-093424_256644097","dateCreated":"2018-12-03T12:12:21+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13390"},{"title":"Loading from the grid as DataFrame and register as view - in order to use sql in later paragraphs","text":"%spark\nval df = spark.read.grid[DelayRec]\ndf.printSchema()\n\n\ndf.createOrReplaceTempView(\"DelayRecView\") // for sark sql needed only for data that is not in space.\n\nval notDelayed = df.filter(df(\"depDelay\") < 15).count() //example for filter\n","user":"anonymous","dateUpdated":"2018-12-13T13:35:01+0200","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [crsDepTime: int, date: string ... 10 more fields]\nroot\n |-- crsDepTime: integer (nullable = true)\n |-- date: string (nullable = true)\n |-- dayOfMonth: integer (nullable = true)\n |-- dayOfWeek: integer (nullable = true)\n |-- depDelay: integer (nullable = true)\n |-- distance: double (nullable = false)\n |-- flightNumber: string (nullable = true)\n |-- hour: integer (nullable = false)\n |-- id: string (nullable = true)\n |-- month: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- year: integer (nullable = true)\n\r\n\nnotDelayed: Long = 238659\n"}]},"apps":[],"jobName":"paragraph_1543831941733_144864450","id":"20170620-093540_604441252","dateCreated":"2018-12-03T12:12:21+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13391"},{"title":"Query Grid Data","text":"%insightedge_jdbc\n\n\nselect dayOfWeek, case when depDelay > 15 then 'delayed' else 'ok' end , count(1)\nfrom DelayRec\ngroup by dayOfWeek , case when depDelay > 15 then 'delayed' else 'ok' end \n\n","user":"anonymous","dateUpdated":"2018-12-23T13:41:56+0200","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"setting":{"multiBarChart":{"stacked":false,"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"dayOfWeek","index":0,"aggr":"sum"}],"groups":[{"name":"EXPR$1","index":1,"aggr":"sum"}],"values":[{"name":"EXPR$2","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"dayOfWeek\tEXPR$1\tEXPR$2\n7\tok     \t33894\n6\tok     \t33019\n3\tdelayed\t14663\n2\tdelayed\t14392\n1\tdelayed\t16238\n7\tdelayed\t14363\n6\tdelayed\t10527\n5\tdelayed\t15552\n4\tdelayed\t15976\n1\tok     \t35121\n5\tok     \t35102\n4\tok     \t34047\n3\tok     \t35282\n2\tok     \t35304\n"}]},"apps":[],"jobName":"paragraph_1543831941737_-1316592328","id":"20180103-085703_2003026011","dateCreated":"2018-12-03T12:12:21+0200","dateStarted":"2018-12-09T13:51:14+0200","dateFinished":"2018-12-09T13:51:19+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13392"},{"text":"%insightedge_jdbc\r\n\r\nselect cast( cast(crsDepTime as int) / 100 as int),  case when depDelay > 15 then 'delayed' else 'ok' end as delay, count(1) \r\nfrom  DelayRec \r\ngroup by  cast( cast(crsDepTime as int) / 100 as int),  case when depDelay > 15 then 'delayed' else 'ok' end\r\n","user":"anonymous","dateUpdated":"2018-12-23T13:41:56+0200","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"setting":{"multiBarChart":{"stacked":false,"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"EXPR$0","index":0,"aggr":"sum"}],"groups":[{"name":"delay","index":1,"aggr":"sum"}],"values":[{"name":"EXPR$2","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"EXPR$0\tdelay\tEXPR$2\n8\tok     \t19872\n6\tok     \t15146\n12\tok     \t12453\n21\tdelayed\t5113\n10\tok     \t16781\n16\tok     \t14350\n14\tok     \t12685\n20\tok     \t13299\n18\tok     \t11984\n11\tdelayed\t5060\n9\tdelayed\t6131\n22\tok     \t1384\n7\tdelayed\t3190\n5\tdelayed\t69\n19\tdelayed\t9409\n17\tdelayed\t8448\n15\tdelayed\t7406\n5\tok     \t803\n13\tdelayed\t7540\n9\tok     \t22622\n7\tok     \t18597\n13\tok     \t19773\n22\tdelayed\t685\n11\tok     \t14777\n20\tdelayed\t10099\n17\tok     \t13154\n15\tok     \t14006\n21\tok     \t7707\n19\tok     \t12376\n10\tdelayed\t5144\n8\tdelayed\t3914\n6\tdelayed\t1921\n18\tdelayed\t8789\n16\tdelayed\t8264\n14\tdelayed\t6140\n12\tdelayed\t4389\n"}]},"apps":[],"jobName":"paragraph_1543831941740_-1244670697","id":"20180103-091807_546038974","dateCreated":"2018-12-03T12:12:21+0200","dateStarted":"2018-12-09T13:57:25+0200","dateFinished":"2018-12-09T13:57:29+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13393"},{"title":"Loading RDD with SQL","text":"%spark\nval sqlRdd = sc.gridSql[DelayRec](\"depDelay > ?\", Seq(15))\nval count = sqlRdd.count()\n","user":"anonymous","dateUpdated":"2018-12-09T13:58:45+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sqlRdd: org.insightedge.spark.rdd.InsightEdgeSqlRDD[common.DelayRec] = InsightEdgeSqlRDD[17] at RDD at InsightEdgeAbstractRDD.scala:36\ncount: Long = 101711\n"}]},"apps":[],"jobName":"paragraph_1543831941744_-1544046175","id":"20170620-093458_2093934595","dateCreated":"2018-12-03T12:12:21+0200","dateStarted":"2018-12-09T13:58:45+0200","dateFinished":"2018-12-09T13:58:49+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13394"},{"title":"Create a Machine learning model based on grid data and save the model to the grid","text":"%spark\n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n\n def gen_features(rec: DelayRec): (String, Array[Double]) = {\n      val values = Array(\n        rec.getDepDelay().toDouble,\n        rec.getMonth().toDouble,\n        rec.getDayOfMonth().toDouble,\n        rec.getDayOfWeek().toDouble,\n        rec.getHour().toDouble,\n        rec.getDistance().toDouble\n      )\n      new Tuple2(rec.getId(), values)\n    }\n    \n def parseData(vals: Array[Double]): LabeledPoint = {\n        LabeledPoint(if (vals(0)>=15) 1.0 else 0.0, Vectors.dense(vals.drop(1)))\n    }\n\n// Load rdd from the grid\nval delayRecRdd = sc.gridRdd[DelayRec]()\n\n// Prepare training set\nval data_2007 = delayRecRdd.map(delayRec => gen_features(delayRec)._2)\nval parsedTrainData = data_2007.map(parseData)\nparsedTrainData.cache\nval scaler = new StandardScaler(withMean = true, withStd = true).fit(parsedTrainData.map(x => x.features))\nval scaledTrainData = parsedTrainData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))\nscaledTrainData.cache\n\n\n\nscaledTrainData.take(3).map(x => (x.label, x.features)).foreach(println)\n\n\n// Build the Logistic Regression model (numIteration should be much higher for accurecy)\nval model_lr = LogisticRegressionWithSGD.train(scaledTrainData, numIterations=20)\n\n\n///ToDo test the model agianst test data, improve using other algo if required and afterward save it to the grid\n\n//Save the model to the grid\nmodel_lr.saveToGrid(sc, \"DelayModel\")\n\n","user":"anonymous","dateUpdated":"2018-12-09T15:11:58+0200","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\ngen_features: (rec: common.DelayRec)(String, Array[Double])\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\ndelayRecRdd: org.insightedge.spark.rdd.InsightEdgeRDD[common.DelayRec] = InsightEdgeRDD[18] at RDD at InsightEdgeAbstractRDD.scala:36\ndata_2007: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[19] at map at <console>:51\nparsedTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[20] at map at <console>:53\nres22: parsedTrainData.type = MapPartitionsRDD[20] at map at <console>:53\nscaler: org.apache.spark.mllib.feature.StandardScalerModel = org.apache.spark.mllib.feature.StandardScalerModel@1deec507\nscaledTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[23] at map at <console>:57\nres23: scaledTrainData.type = MapPartitionsRDD[23] at map at <console>:57\n(0.0,[1.6159597680040167,-0.19588287200627402,0.5314021520838096,-0.0782980054036891,1.0738427064690959])\r\n(0.0,[1.6159597680040167,-0.7645172984324486,1.5332546905137328,-1.4004995063450794,1.0738427064690959])\r\n(0.0,[1.6159597680040167,-1.3331517248586233,-0.9713766555610753,0.803169661890571,-0.1705106898643185])\r\nwarning: there was one deprecation warning; re-run with -deprecation for details\nmodel_lr: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 5, numClasses = 2, threshold = 0.5\n"}]},"apps":[],"jobName":"paragraph_1543831941749_-1893495400","id":"20180104-104737_795828311","dateCreated":"2018-12-03T12:12:21+0200","dateStarted":"2018-12-09T15:11:58+0200","dateFinished":"2018-12-09T15:12:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13395"},{"text":"%md\nAt this stage we want to simulate a new data and predict if new flight is going to be delayed according to the model,We can either\nput new data in kafka and read here the data from the stream related to newFlights topic, or read a csv with the new data.\n","user":"anonymous","dateUpdated":"2018-12-17T11:55:45+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>At this stage we want to simulate a new data and predict if new flight is going to be delayed according to the model,We can either<br/>put new data in kafka and read here the data from the stream related to newFlights topic, or read a csv with the new data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1544100640939_1704690511","id":"20181206-145040_381020263","dateCreated":"2018-12-06T14:50:40+0200","dateStarted":"2018-12-17T11:55:45+0200","dateFinished":"2018-12-17T11:55:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13396"},{"title":"Simulate new data reading CSV and save it to the grid","text":"%spark\nimport common.DelayRec\nimport org.insightedge.spark.implicits.all._\n\n\nval data = sc.textFile(\"C:/gs/12.3/2008.csv\").map(line => line.split(\",\")).filter(rec => rec(4).toString() != \"NA\").filter(rec => rec(0) !=\"Year\").filter(rec => rec(21).toString() != \"1\").filter(rec => rec(16).toString() == \"ORD\").filter(rec => rec(1).toString() == \"1\").filter(rec => rec(2).toString() == \"3\").map(rec => new DelayRec( rec(9).toString(),rec(0).toInt,rec(1).toInt,rec(2).toInt,rec(3).toInt,rec(5).toInt,rec(15).toInt,rec(16).toString(),rec(18).toDouble))\n   \n//see how the data looks like    \ndata.take(5).map(x => x.toString()).foreach(println)\n\n//save the data to the grid (routing is flight number, id is flight+date)\ndata.saveToGrid()","user":"anonymous","dateUpdated":"2018-12-09T15:13:59+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport common.DelayRec\n\nimport org.insightedge.spark.implicits.all._\n\ndata: org.apache.spark.rdd.RDD[common.DelayRec] = MapPartitionsRDD[74] at map at <console>:57\ncommon.DelayRec@2a27a288\r\ncommon.DelayRec@22798149\r\ncommon.DelayRec@4b492b46\r\ncommon.DelayRec@2e8db5f7\r\ncommon.DelayRec@3929614c\r\n"}]},"apps":[],"jobName":"paragraph_1543831941751_-1345645509","id":"20180108-130822_1347993732","dateCreated":"2018-12-03T12:12:21+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13397"},{"title":"Read new data from kafka","text":"%spark \nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport common.DelayRec\n \nval ssc = new StreamingContext(sc, Seconds(10))\n\nssc.checkpoint(\"c:/ie-training/tmp\")\n \nval kafkaParams = Map(\n    \"metadata.broker.list\" -> \"localhost:9092\",\n    \"auto.offset.reset\" -> \"smallest\"\n)\n \nval topics = Set(\"newFlights\")\n \nval messagesStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)\n  \nval data = messagesStream.foreachRDD(rdd => rdd..map(line => line.split(\",\")).filter(rec => rec(4).toString() != \"NA\").filter(rec => rec(0) !=\"Year\").filter(rec => rec(16).toString() == \"ORD\").filter(rec => rec(21).toString() != \"1\").map(rec => new DelayRec( rec(9).toString(),rec(0).toInt,rec(1).toInt,rec(2).toInt,rec(3).toInt,rec(5).toInt,rec(15).toInt,rec(16).toString(),rec(18).toDouble))\n\n\nssc.start \n","user":"anonymous","dateUpdated":"2018-12-13T13:29:39+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544700362440_917010027","id":"20181213-132602_1351345709","dateCreated":"2018-12-13T13:26:02+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:13398"},{"title":"Stop streaming context","text":"%spark\nssc.stop(stopSparkContext=false, stopGracefully=true)","user":"anonymous","dateUpdated":"2018-12-13T13:33:07+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544361239412_1555762326","id":"20181209-151359_1079697451","dateCreated":"2018-12-09T15:13:59+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:13399"},{"text":"%md\nAt this stage we will load machine learnning model from the grid\nWe will also load the new information(in this case the 2008 data) and predict if scheduled flights are going to be delayed, In the grid itselef there is a notify container waiting for those predictions that can trigger logics in case of expected delay.","user":"anonymous","dateUpdated":"2018-12-17T12:00:23+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>At this stage we will load machine learnning model from the grid<br/>We will also load the new information(in this case the 2008 data) and predict if scheduled flights are going to be delayed, In the grid itselef there is a notify container waiting for those predictions that can trigger logics in case of expected delay.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545040600449_685077288","id":"20181217-115640_404323044","dateCreated":"2018-12-17T11:56:40+0200","dateStarted":"2018-12-17T12:00:23+0200","dateFinished":"2018-12-17T12:00:23+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13400"},{"title":"Predict according to model, save prediction objects to grid","text":"%spark\nimport common.FlightDelayed\nimport org.insightedge.spark.implicits.all._\nimport org.apache.spark.mllib.classification.LogisticRegressionModel\n\n//load the model from the grid\nval model = sc.loadMLInstance[LogisticRegressionModel](\"DelayModel\").get\n\n//Load new data(2008) from the grid\nval newdata = sc.gridSql[DelayRec](\"year = ?\", Seq(2008))\nnewdata.count\n\n// Prepare the new data\n\ndef gen_features(rec: DelayRec): (String, Array[Double]) = {\n      val values = Array(\n        rec.getDepDelay().toDouble,\n        rec.getMonth().toDouble,\n        rec.getDayOfMonth().toDouble,\n        rec.getDayOfWeek().toDouble,\n        rec.getHour().toDouble,\n        rec.getDistance().toDouble\n      )\n      new Tuple2(rec.getId(), values)\n    }\n    \n def parseData(vals: Array[Double]): LabeledPoint = {\n        LabeledPoint(if (vals(0)>=15) 1.0 else 0.0, Vectors.dense(vals.drop(1)))\n    }\n\n\n\nval parsedNewData = newdata.map(delayRec => gen_features(delayRec)._2).map(parseData)\nparsedNewData.cache\nval scaledNewData = parsedNewData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))\nscaledNewData.cache\n\n//predict\nval preds_lr = scaledNewData.map(point => model.predict(point.features)) \n  \n\npreds_lr.count()\n\n//zip together data and prediction filter only delayed predictions (prediction 1.0)\nval zipped = newdata.zip(preds_lr).filter(rec => rec._2 == 1.0)\n\n \n//Report to the grid on flight delay predictions\nval predictions = zipped.map { rec => new FlightDelayed(rec._1.getId(), true)}\npredictions.saveToGrid()\n\n","user":"anonymous","dateUpdated":"2018-12-17T12:01:08+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport common.FlightDelayed\n\nimport org.insightedge.spark.implicits.all._\n\nimport org.apache.spark.mllib.classification.LogisticRegressionModel\n\nmodel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 5, numClasses = 2, threshold = 0.5\n\nnewdata: org.insightedge.spark.rdd.InsightEdgeSqlRDD[common.DelayRec] = InsightEdgeSqlRDD[75] at RDD at InsightEdgeAbstractRDD.scala:36\n\nres45: Long = 952\n\ngen_features: (rec: common.DelayRec)(String, Array[Double])\n\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\n\nparsedNewData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[77] at map at <console>:63\n\nres53: parsedNewData.type = MapPartitionsRDD[77] at map at <console>:63\n\nscaledNewData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[78] at map at <console>:74\n\nres54: scaledNewData.type = MapPartitionsRDD[78] at map at <console>:74\n\npreds_lr: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[79] at map at <console>:80\n\nres59: Long = 952\n\nzipped: org.apache.spark.rdd.RDD[(common.DelayRec, Double)] = MapPartitionsRDD[81] at filter at <console>:82\n\npredictions: org.apache.spark.rdd.RDD[common.FlightDelayed] = MapPartitionsRDD[82] at map at <console>:85\n"}]},"apps":[],"jobName":"paragraph_1543831941754_577464914","id":"20180103-092239_1051986871","dateCreated":"2018-12-03T12:12:21+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13401"},{"text":"%md\n## Congratulations, it's done.\n##### You can create your own notebook in 'Notebook' menu. Good luck!\n","user":"anonymous","dateUpdated":"2018-12-03T12:12:21+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Congratulations, it&rsquo;s done.</h2>\n<h5>You can create your own notebook in &lsquo;Notebook&rsquo; menu. Good luck!</h5>\n</div>"}]},"apps":[],"jobName":"paragraph_1543831941757_-1584134725","id":"20170620-093845_67106827","dateCreated":"2018-12-03T12:12:21+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13402"}],"name":"Flight Delay Prediction example","id":"2DWTUT7BB","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"insightedge_jdbc:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}