{
  "paragraphs": [
    {
      "text": "%md\n\nThis is an example of using Spark to read from Kafka.\n",
      "user": "anonymous",
      "dateUpdated": "2019-02-26T17:45:27-0500",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1551220553787_1812601693",
      "id": "20190226-173553_1419260237",
      "dateCreated": "2019-02-26T17:35:53-0500",
      "dateStarted": "2019-02-26T17:45:27-0500",
      "dateFinished": "2019-02-26T17:45:27-0500",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:5088"
    },
    {
      "text": "%dep\n\nz.load(\"org.apache.spark:spark-streaming-kafka-0-10_2.11:jar:2.3.2\")\n",
      "user": "anonymous",
      "dateUpdated": "2019-02-26T17:45:15-0500",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1551127148926_-284389585",
      "id": "20190225-153908_262979671",
      "dateCreated": "2019-02-25T15:39:08-0500",
      "dateStarted": "2019-02-26T17:45:15-0500",
      "dateFinished": "2019-02-26T17:45:25-0500",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:5089"
    },
    {
      "text": "%spark\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.streaming._\n\n//sc.setLogLevel(\"ERROR\")  // prevent INFO logging from polluting output\n\nval ssc =  StreamingContext.getActiveOrCreate(() => new StreamingContext(sc, Seconds(5)))    // creating the StreamingContext with 5 seconds interval\nval kafkaParams = Map[String, Object](\n  \"bootstrap.servers\" -> \"localhost:9092\",\n  \"key.deserializer\" -> classOf[StringDeserializer],\n  \"value.deserializer\" -> classOf[StringDeserializer],\n  \"group.id\" -> \"my_group_id\",\n  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n)\n\nval topics = Array(\"messages\")\nval messages = KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](topics, kafkaParams)\n)\n\nmessages.map(record=>(record.value().toString)).print\n\n/*\nmessages.foreachRDD { rdd =>\n      System.out.println(\"--- New RDD with \" + rdd.partitions.size + \" partitions and \" + rdd.count + \" records\")\n      rdd.foreach { record =>\n        System.out.println(record.value())\n      }\n    }\n    \n*/    \nssc.start()\n\nssc.awaitTermination()\n",
      "user": "anonymous",
      "dateUpdated": "2019-02-26T17:45:36-0500",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1551130857342_1592332181",
      "id": "20190225-164057_2132953088",
      "dateCreated": "2019-02-25T16:40:57-0500",
      "dateStarted": "2019-02-26T17:45:36-0500",
      "dateFinished": "2019-02-26T17:47:03-0500",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:5090"
    },
    {
      "text": "%spark\n\n\nssc.stop(stopSparkContext=false, stopGracefully=true)\n",
      "user": "anonymous",
      "dateUpdated": "2019-02-26T17:42:31-0500",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1551127232282_959785755",
      "id": "20190225-154032_698296833",
      "dateCreated": "2019-02-25T15:40:32-0500",
      "dateStarted": "2019-02-25T17:38:06-0500",
      "dateFinished": "2019-02-25T17:39:05-0500",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:5091"
    }
  ],
  "name": "Lab 11.4 Example - Kafka Streaming",
  "id": "2E7CBD6NP",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}