{
  "paragraphs": [
    {
      "title": "K-means",
      "text": "%md\nsee http://spark.apache.org/docs/latest/mllib-clustering.html\nK-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters.\n\nparameters:\nk is the number of desired clusters\nmaxIterations is the maximum number of iterations to run.\ninitializationMode specifies either random initialization or initialization via k-means.\ninitializationSteps determines the number of steps in the k-means algorithm.\nepsilon determines the distance threshold within which we consider k-means to have converged.\ninitialModel is an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed.",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:26:22+0300",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556801420918_1229671753",
      "id": "20181224-140936_2145395395",
      "dateCreated": "2019-05-02T15:50:20+0300",
      "dateStarted": "2019-05-02T16:26:22+0300",
      "dateFinished": "2019-05-02T16:26:22+0300",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:1340",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>see <a href=\"http://spark.apache.org/docs/latest/mllib-clustering.html\">http://spark.apache.org/docs/latest/mllib-clustering.html</a><br/>K-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters.</p>\n<p>parameters:<br/>k is the number of desired clusters<br/>maxIterations is the maximum number of iterations to run.<br/>initializationMode specifies either random initialization or initialization via k-means.<br/>initializationSteps determines the number of steps in the k-means algorithm.<br/>epsilon determines the distance threshold within which we consider k-means to have converged.<br/>initialModel is an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed.</p>\n</div>"
          }
        ]
      }
    },
    {
      "title": "K-Means to create a model of spam/valid emails",
      "text": "%spark\nimport org.apache.spark.ml.clustering._\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.insightedge.spark.implicits.all._\nimport sqlContext.implicits._\n\n\n\nfinal case class Email(id: BigInt, text: String)\n\n\nval emails = Seq(\n\"Dear Spark Learner, Thanks so much for attending the Spark Summit 2014!  Check out videos of talks from the summit at\",\n\"Hi Mom, Apologies for being late about emailing and forgetting to send you the package.  I hope you and bro have been\",\n\"Wow, hey Fred, just heard about the Spark petabyte sort.  I think we need to take time to try it out immediately\",\n\"Hi Spark user list, This is my first question to this list, so thanks in advance for your help!  I tried running\",\n\"Thanks Tom for your email.  I need to refer you to Alice for this one.  I haven't yet figured out that part either\",\n\"Hi Bob, Good job yesterday!  I was attending your talk, and really enjoyed it.  I want to try out GraphX\",\n\"Hi Bob, Summit demo got whoops from audience!  Had to let you know.  Regards Joe\",\n\"24 Hour Flash Sale: up to 50% off, order by 12/3 x Wine Country Gift Baskets info@message.winecountrygiftbaskets you will get money back\",\n\"Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please\",\n\"Get Viagra real cheap!  Send money right away to \",\n\"Oh my gosh you can be really strong too with these drugs found in the rainforest. Get them cheap right now\",\n\"YOUR COMPUTER HAS BEEN INFECTED!  YOU MUST RESET YOUR PASSWORD.  Reply to this email with your password and SSN\",\n\"THIS IS NOT A SCAM!  Send money and get access to awesome stuff really cheap and never have to \").zipWithIndex.map(_.swap).toDF(\"id\", \"text\").as[Email]\n\n// Prepare data for k-means\n// Pass emails through a \"pipeline\" of transformers\n\nval tok = new RegexTokenizer().setInputCol(\"text\").setOutputCol(\"tokens\").setPattern(\"\\\\W+\")\n\nval hashTF = new HashingTF().setInputCol(\"tokens\").setOutputCol(\"features\").setNumFeatures(100)\n\nval preprocess = (tok.transform _).andThen(hashTF.transform)\n\nval features = preprocess(emails.toDF)\n\nfeatures.select('text, 'features).show(false)\n\n//(max defult number of iterations is 10 )\nval kmeans = new KMeans().setK(2)\n\nval kmModel = kmeans.fit(features.toDF)\nkmModel.clusterCenters.map(_.toSparse)\nval email = Seq(\"hello mom\").toDF(\"text\")\nval result = kmModel.transform(preprocess(email)).show(false)\n\n\nkmModel.saveToGrid(sc, \"emailsKModel\")\nval savedModel = sc.loadMLInstance[KMeansModel](\"emailsKModel\").get\n",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:22:56+0300",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.222:4040/jobs/job?id=103",
            "http://192.168.9.222:4040/jobs/job?id=104",
            "http://192.168.9.222:4040/jobs/job?id=105",
            "http://192.168.9.222:4040/jobs/job?id=106",
            "http://192.168.9.222:4040/jobs/job?id=107",
            "http://192.168.9.222:4040/jobs/job?id=108",
            "http://192.168.9.222:4040/jobs/job?id=109",
            "http://192.168.9.222:4040/jobs/job?id=110",
            "http://192.168.9.222:4040/jobs/job?id=111",
            "http://192.168.9.222:4040/jobs/job?id=112"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1556801420924_-1555422463",
      "id": "20181224-141330_991230917",
      "dateCreated": "2019-05-02T15:50:20+0300",
      "dateStarted": "2019-05-02T16:22:56+0300",
      "dateFinished": "2019-05-02T16:23:08+0300",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:1341"
    },
    {
      "title": "Logistic regression",
      "text": "%md\nsee http://spark.apache.org/docs/latest/ml-classification-regression.html\nLogistic regression is a popular method to predict a categorical response. It is a special case of Generalized Linear models that predicts the probability of the outcomes. In spark.ml logistic regression can be used to predict a binary outcome by using binomial logistic regression, or it can be used to predict a multiclass outcome by using multinomial logistic regression.",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:26:25+0300",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556801420926_283524511",
      "id": "20181224-083519_149685599",
      "dateCreated": "2019-05-02T15:50:20+0300",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:1342",
      "dateFinished": "2019-05-02T16:26:25+0300",
      "dateStarted": "2019-05-02T16:26:25+0300",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>see <a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html\">http://spark.apache.org/docs/latest/ml-classification-regression.html</a><br/>Logistic regression is a popular method to predict a categorical response. It is a special case of Generalized Linear models that predicts the probability of the outcomes. In spark.ml logistic regression can be used to predict a binary outcome by using binomial logistic regression, or it can be used to predict a multiclass outcome by using multinomial logistic regression.</p>\n</div>"
          }
        ]
      }
    },
    {
      "title": "Binomial logistic regression - to model spam and valid emails",
      "text": "%spark\r\n\r\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\r\nimport org.insightedge.spark.implicits.all._\r\nimport org.apache.spark.mllib.feature._\r\nimport org.apache.spark.mllib.regression.LabeledPoint\r\n\r\n\r\n\r\n    // Load 2 types of emails from text files: spam and ham (non-spam).\r\n    // Each line has text from one email.\r\n    // In this case we split the emails Seq from previous example to two groups one that we know to be spam and another with is valid\r\n    val spam = sc.textFile(\"../../../../Data/spam.txt\")\r\n    val ham = sc.textFile(\"../../../../Data/validmail.txt\")\r\n\r\n    // Create a HashingTF instance to map email text to vectors of 100 features.\r\n    val tf = new HashingTF(numFeatures = 100)\r\n    // Each email is split into words, and each word is mapped to one feature.\r\n    val spamFeatures = spam.map(email => tf.transform(email.split(\" \")))\r\n    val hamFeatures = ham.map(email => tf.transform(email.split(\" \")))\r\n\r\n    // Create LabeledPoint datasets for spam labled 1 and valid labled 0 examples.\r\n    val positiveExamples = spamFeatures.map(features => LabeledPoint(1, features))\r\n    val negativeExamples = hamFeatures.map(features => LabeledPoint(0, features))\r\n    val trainingData = positiveExamples ++ negativeExamples\r\n    trainingData.cache() // Cache data since Logistic Regression is an iterative algorithm.\r\n\r\n    // Create a Logistic Regression learner which uses the LBFGS optimizer.\r\n    val lrLearner = new LogisticRegressionWithSGD()\r\n    // Run the actual learning algorithm on the training data.\r\n    val model = lrLearner.run(trainingData)\r\n    model.saveToGrid(sc, \"emailsLRModel\")\r\n    \r\n    \r\n\r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:03:05+0300",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.222:4040/jobs/job?id=0",
            "http://192.168.9.222:4040/jobs/job?id=1",
            "http://192.168.9.222:4040/jobs/job?id=2",
            "http://192.168.9.222:4040/jobs/job?id=3",
            "http://192.168.9.222:4040/jobs/job?id=4",
            "http://192.168.9.222:4040/jobs/job?id=5",
            "http://192.168.9.222:4040/jobs/job?id=6",
            "http://192.168.9.222:4040/jobs/job?id=7",
            "http://192.168.9.222:4040/jobs/job?id=8",
            "http://192.168.9.222:4040/jobs/job?id=9",
            "http://192.168.9.222:4040/jobs/job?id=10",
            "http://192.168.9.222:4040/jobs/job?id=11",
            "http://192.168.9.222:4040/jobs/job?id=12",
            "http://192.168.9.222:4040/jobs/job?id=13",
            "http://192.168.9.222:4040/jobs/job?id=14",
            "http://192.168.9.222:4040/jobs/job?id=15",
            "http://192.168.9.222:4040/jobs/job?id=16",
            "http://192.168.9.222:4040/jobs/job?id=17",
            "http://192.168.9.222:4040/jobs/job?id=18",
            "http://192.168.9.222:4040/jobs/job?id=19",
            "http://192.168.9.222:4040/jobs/job?id=20",
            "http://192.168.9.222:4040/jobs/job?id=21",
            "http://192.168.9.222:4040/jobs/job?id=22",
            "http://192.168.9.222:4040/jobs/job?id=23",
            "http://192.168.9.222:4040/jobs/job?id=24",
            "http://192.168.9.222:4040/jobs/job?id=25",
            "http://192.168.9.222:4040/jobs/job?id=26",
            "http://192.168.9.222:4040/jobs/job?id=27",
            "http://192.168.9.222:4040/jobs/job?id=28",
            "http://192.168.9.222:4040/jobs/job?id=29",
            "http://192.168.9.222:4040/jobs/job?id=30",
            "http://192.168.9.222:4040/jobs/job?id=31",
            "http://192.168.9.222:4040/jobs/job?id=32",
            "http://192.168.9.222:4040/jobs/job?id=33",
            "http://192.168.9.222:4040/jobs/job?id=34",
            "http://192.168.9.222:4040/jobs/job?id=35",
            "http://192.168.9.222:4040/jobs/job?id=36",
            "http://192.168.9.222:4040/jobs/job?id=37",
            "http://192.168.9.222:4040/jobs/job?id=38",
            "http://192.168.9.222:4040/jobs/job?id=39",
            "http://192.168.9.222:4040/jobs/job?id=40",
            "http://192.168.9.222:4040/jobs/job?id=41",
            "http://192.168.9.222:4040/jobs/job?id=42",
            "http://192.168.9.222:4040/jobs/job?id=43",
            "http://192.168.9.222:4040/jobs/job?id=44",
            "http://192.168.9.222:4040/jobs/job?id=45",
            "http://192.168.9.222:4040/jobs/job?id=46",
            "http://192.168.9.222:4040/jobs/job?id=47",
            "http://192.168.9.222:4040/jobs/job?id=48",
            "http://192.168.9.222:4040/jobs/job?id=49",
            "http://192.168.9.222:4040/jobs/job?id=50",
            "http://192.168.9.222:4040/jobs/job?id=51",
            "http://192.168.9.222:4040/jobs/job?id=52",
            "http://192.168.9.222:4040/jobs/job?id=53",
            "http://192.168.9.222:4040/jobs/job?id=54",
            "http://192.168.9.222:4040/jobs/job?id=55",
            "http://192.168.9.222:4040/jobs/job?id=56",
            "http://192.168.9.222:4040/jobs/job?id=57",
            "http://192.168.9.222:4040/jobs/job?id=58",
            "http://192.168.9.222:4040/jobs/job?id=59",
            "http://192.168.9.222:4040/jobs/job?id=60",
            "http://192.168.9.222:4040/jobs/job?id=61",
            "http://192.168.9.222:4040/jobs/job?id=62",
            "http://192.168.9.222:4040/jobs/job?id=63",
            "http://192.168.9.222:4040/jobs/job?id=64",
            "http://192.168.9.222:4040/jobs/job?id=65",
            "http://192.168.9.222:4040/jobs/job?id=66",
            "http://192.168.9.222:4040/jobs/job?id=67",
            "http://192.168.9.222:4040/jobs/job?id=68",
            "http://192.168.9.222:4040/jobs/job?id=69",
            "http://192.168.9.222:4040/jobs/job?id=70",
            "http://192.168.9.222:4040/jobs/job?id=71",
            "http://192.168.9.222:4040/jobs/job?id=72",
            "http://192.168.9.222:4040/jobs/job?id=73",
            "http://192.168.9.222:4040/jobs/job?id=74",
            "http://192.168.9.222:4040/jobs/job?id=75",
            "http://192.168.9.222:4040/jobs/job?id=76",
            "http://192.168.9.222:4040/jobs/job?id=77",
            "http://192.168.9.222:4040/jobs/job?id=78",
            "http://192.168.9.222:4040/jobs/job?id=79",
            "http://192.168.9.222:4040/jobs/job?id=80",
            "http://192.168.9.222:4040/jobs/job?id=81",
            "http://192.168.9.222:4040/jobs/job?id=82",
            "http://192.168.9.222:4040/jobs/job?id=83",
            "http://192.168.9.222:4040/jobs/job?id=84",
            "http://192.168.9.222:4040/jobs/job?id=85",
            "http://192.168.9.222:4040/jobs/job?id=86",
            "http://192.168.9.222:4040/jobs/job?id=87",
            "http://192.168.9.222:4040/jobs/job?id=88",
            "http://192.168.9.222:4040/jobs/job?id=89",
            "http://192.168.9.222:4040/jobs/job?id=90",
            "http://192.168.9.222:4040/jobs/job?id=91",
            "http://192.168.9.222:4040/jobs/job?id=92",
            "http://192.168.9.222:4040/jobs/job?id=93",
            "http://192.168.9.222:4040/jobs/job?id=94",
            "http://192.168.9.222:4040/jobs/job?id=95",
            "http://192.168.9.222:4040/jobs/job?id=96",
            "http://192.168.9.222:4040/jobs/job?id=97",
            "http://192.168.9.222:4040/jobs/job?id=98",
            "http://192.168.9.222:4040/jobs/job?id=99",
            "http://192.168.9.222:4040/jobs/job?id=100",
            "http://192.168.9.222:4040/jobs/job?id=101",
            "http://192.168.9.222:4040/jobs/job?id=102"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1556801420927_-1365023144",
      "id": "20181223-142523_699602601",
      "dateCreated": "2019-05-02T15:50:20+0300",
      "dateStarted": "2019-05-02T16:03:05+0300",
      "dateFinished": "2019-05-02T16:03:26+0300",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:1343"
    },
    {
      "title": "Load models from space , predict if email is spam based on the 2 modeles",
      "text": "%spark\nimport org.apache.spark.ml.clustering.KMeansModel\nimport org.apache.spark.mllib.classification.LogisticRegressionModel\nimport org.apache.spark.ml.clustering._\nimport org.apache.spark.ml.feature._\n\nval emailsKModel = sc.loadMLInstance[KMeansModel](\"emailsKModel\").get\nval emailsLRModel = sc.loadMLInstance[LogisticRegressionModel](\"emailsLRModel\").get\n\nval spamTest = \"O M G GET cheap stuff by sending money to \"\nval validTest = \"Hi Dad, I started studying Spark and I had to tell you \"\n\n// transform and test new  data on LogisticRegressionModel\nval spamPredict = emailsLRModel.predict(tf.transform(spamTest.split(\" \")))\nval validPredict = emailsLRModel.predict(tf.transform(validTest.split(\" \")))\n\n// transfom and test new data on KMeansModel\n// add tranformation \nval tok = new RegexTokenizer().setInputCol(\"text\").setOutputCol(\"tokens\").setPattern(\"\\\\W+\")\nval hashTF = new HashingTF().setInputCol(\"tokens\").setOutputCol(\"features\").setNumFeatures(100)\nval preprocess = (tok.transform _).andThen(hashTF.transform)\n\nval spamTest4KMeans = preprocess(Seq(spamTest).toDF(\"text\"))\nval validTest4KMeans = preprocess(Seq(validTest).toDF(\"text\")) \nval spamresult = emailsKModel.transform(spamTest4KMeans).select(\"prediction\").show()\nval validesult = emailsKModel.transform(validTest4KMeans).select(\"prediction\").show()\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:24:14+0300",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556801420929_1331079820",
      "id": "20181224-085220_158226380",
      "dateCreated": "2019-05-02T15:50:20+0300",
      "dateStarted": "2019-05-02T16:24:14+0300",
      "dateFinished": "2019-05-02T16:24:20+0300",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:1344"
    },
    {
      "title": "Multiclass classification",
      "text": "%md\nMulticlass classification is supported via multinomial logistic (softmax) regression. In multinomial logistic regression, the algorithm produces K sets of coefficients, or a matrix of dimension K×J where K is the number of outcome classes and J is the number of features. If the algorithm is fit with an intercept term then a length K vector of intercepts is available.\n\nMultinomial coefficients are available as coefficientMatrix and intercepts are available as interceptVector.\n\nThe conditional probabilities of the outcome classes k∈1,2,…,K are modeled using the softmax function.\n\nP(Y=k|X,βk,β0k)=eβk⋅X+β0k∑K−1k′=0eβk′⋅X+β0k′\n\nWe minimize the weighted negative log-likelihood, using a multinomial response model, with elastic-net penalty to control for overfitting.\n\nminβ,β0−[∑i=1Lwi⋅logP(Y=yi|xi)]+λ[12(1−α)||β||22+α||β||1]\n",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:26:30+0300",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556801420930_-508554916",
      "id": "20181224-085216_1553489958",
      "dateCreated": "2019-05-02T15:50:20+0300",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:1345",
      "dateFinished": "2019-05-02T16:26:30+0300",
      "dateStarted": "2019-05-02T16:26:30+0300",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Multiclass classification is supported via multinomial logistic (softmax) regression. In multinomial logistic regression, the algorithm produces K sets of coefficients, or a matrix of dimension K×J where K is the number of outcome classes and J is the number of features. If the algorithm is fit with an intercept term then a length K vector of intercepts is available.</p>\n<p>Multinomial coefficients are available as coefficientMatrix and intercepts are available as interceptVector.</p>\n<p>The conditional probabilities of the outcome classes k∈1,2,…,K are modeled using the softmax function.</p>\n<p>P(Y=k|X,βk,β0k)=eβk⋅X+β0k∑K−1k′=0eβk′⋅X+β0k′</p>\n<p>We minimize the weighted negative log-likelihood, using a multinomial response model, with elastic-net penalty to control for overfitting.</p>\n<p>minβ,β0−[∑i=1Lwi⋅logP(Y=yi|xi)]+λ[12(1−α)||β||22+α||β||1]</p>\n</div>"
          }
        ]
      }
    },
    {
      "title": "Multinomial logistic regression",
      "text": "%spark\nimport org.apache.spark.ml.classification.LogisticRegression\n\n// Load training data\nval training = spark.read.format(\"libsvm\").load(\"../../../../Data/sample_multiclass_classification_data.txt\")\n\nval lr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n\n// Fit the model\nval lrModel = lr.fit(training)\n\n// Print the coefficients and intercept for multinomial logistic regression\nprintln(s\"Coefficients: \\n${lrModel.coefficientMatrix}\")\nprintln(s\"Intercepts: \\n${lrModel.interceptVector}\")\n\nval trainingSummary = lrModel.summary\n\n// Obtain the objective per iteration\nval objectiveHistory = trainingSummary.objectiveHistory\nprintln(\"objectiveHistory:\")\nobjectiveHistory.foreach(println)\n\n// for multiclass, we can inspect metrics on a per-label basis\nprintln(\"False positive rate by label:\")\ntrainingSummary.falsePositiveRateByLabel.zipWithIndex.foreach { case (rate, label) =>\n  println(s\"label $label: $rate\")\n}\n\nprintln(\"True positive rate by label:\")\ntrainingSummary.truePositiveRateByLabel.zipWithIndex.foreach { case (rate, label) =>\n  println(s\"label $label: $rate\")\n}\n\nprintln(\"Precision by label:\")\ntrainingSummary.precisionByLabel.zipWithIndex.foreach { case (prec, label) =>\n  println(s\"label $label: $prec\")\n}\n\nprintln(\"Recall by label:\")\ntrainingSummary.recallByLabel.zipWithIndex.foreach { case (rec, label) =>\n  println(s\"label $label: $rec\")\n}\n\n\nprintln(\"F-measure by label:\")\ntrainingSummary.fMeasureByLabel.zipWithIndex.foreach { case (f, label) =>\n  println(s\"label $label: $f\")\n}\n\nval accuracy = trainingSummary.accuracy\nval falsePositiveRate = trainingSummary.weightedFalsePositiveRate\nval truePositiveRate = trainingSummary.weightedTruePositiveRate\nval fMeasure = trainingSummary.weightedFMeasure\nval precision = trainingSummary.weightedPrecision\nval recall = trainingSummary.weightedRecall\nprintln(s\"Accuracy: $accuracy\\nFPR: $falsePositiveRate\\nTPR: $truePositiveRate\\n\" +\n  s\"F-measure: $fMeasure\\nPrecision: $precision\\nRecall: $recall\")\n  \n//Save the model to the grid  \nlrModel.saveToGrid(sc, \"LRModel2\")\n  \n",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:25:26+0300",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.222:4040/jobs/job?id=113",
            "http://192.168.9.222:4040/jobs/job?id=114",
            "http://192.168.9.222:4040/jobs/job?id=115",
            "http://192.168.9.222:4040/jobs/job?id=116",
            "http://192.168.9.222:4040/jobs/job?id=117",
            "http://192.168.9.222:4040/jobs/job?id=118",
            "http://192.168.9.222:4040/jobs/job?id=119",
            "http://192.168.9.222:4040/jobs/job?id=120",
            "http://192.168.9.222:4040/jobs/job?id=121",
            "http://192.168.9.222:4040/jobs/job?id=122",
            "http://192.168.9.222:4040/jobs/job?id=123",
            "http://192.168.9.222:4040/jobs/job?id=124",
            "http://192.168.9.222:4040/jobs/job?id=125",
            "http://192.168.9.222:4040/jobs/job?id=126",
            "http://192.168.9.222:4040/jobs/job?id=127",
            "http://192.168.9.222:4040/jobs/job?id=128",
            "http://192.168.9.222:4040/jobs/job?id=129",
            "http://192.168.9.222:4040/jobs/job?id=130",
            "http://192.168.9.222:4040/jobs/job?id=131"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1556801420932_1607366810",
      "id": "20181224-083956_1363389928",
      "dateCreated": "2019-05-02T15:50:20+0300",
      "dateStarted": "2019-05-02T16:25:26+0300",
      "dateFinished": "2019-05-02T16:25:29+0300",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:1346"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2019-05-02T16:25:26+0300",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556803526897_-2079065157",
      "id": "20190502-162526_917881679",
      "dateCreated": "2019-05-02T16:25:26+0300",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:1347"
    }
  ],
  "name": "Lab 12.1 Solution - Machine Learning",
  "id": "2E9SPRHAC",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}