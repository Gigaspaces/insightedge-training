{
  "paragraphs": [
    {
      "text": "%md\r\n## RDD Basics\r\n\r\nAn RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.\r\n\r\nUsers create RDDs in two ways:\r\n\r\n* by loading an external dataset \r\n* by distributing a collection of objects (e.g., a list or set) in their driver program.\r\n\r\n\r\n\r\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:20:50+0200",
      "config": {
        "tableHide": false,
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>RDD Basics</h2>\n<p>An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.</p>\n<p>Users create RDDs in two ways:</p>\n<ul>\n  <li>by loading an external dataset</li>\n  <li>by distributing a collection of objects (e.g., a list or set) in their driver program.</li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692281_102944053",
      "id": "20160822-102138_811007240",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "dateStarted": "2018-12-25T15:20:51+0200",
      "dateFinished": "2018-12-25T15:20:55+0200",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "focus": true,
      "$$hashKey": "object:86361"
    },
    {
      "title": "Example of loading RDD from an external dataset",
      "text": "%spark\n\nval lines = spark.sparkContext.textFile(\"../../../../Data/README.md\")\n\nlines.take(5).foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:49:56+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "lines: org.apache.spark.rdd.RDD[String] = ../../../../Data/README.md MapPartitionsRDD[17] at textFile at <console>:23\n# Apache Spark\n\nSpark is a fast and general cluster computing system for Big Data. It provides\nhigh-level APIs in Scala, Java, Python, and R, and an optimized engine that\nsupports general computation graphs for data analysis. It also supports a\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.185:4041/jobs/job?id=3"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1545743692291_-826141926",
      "id": "20160822-103857_523919705",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "dateStarted": "2018-12-25T15:49:56+0200",
      "dateFinished": "2018-12-25T15:49:57+0200",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86362"
    },
    {
      "title": "Example of loading RDD by parallelizing a collection",
      "text": "%spark\n\nval lines = spark.sparkContext.parallelize(List(\"insightedge\", \"i like insightedge\", \"insightedge rocks\"))\n\nlines.collect.foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692293_1107183375",
      "id": "20160822-105959_2005037423",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86363"
    },
    {
      "text": "%md\n## RDD Transformation and Actions\n\nRDDs support two types of operations\n\n* Transformations \n* Actions\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "tableHide": false,
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h2>RDD Transformation and Actions</h2>\n<p>RDDs support two types of operations</p>\n<ul>\n<li>Transformations</li>\n<li>Actions</li>\n</ul>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692294_1808248856",
      "id": "20160822-110526_771394320",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86364"
    },
    {
      "title": "",
      "text": "%md\n\n### **Transformations**\n\n*A transformation method of an RDD creates a new RDD by performing a computation on the source RDD.* \n\nRDD transformations are conceptually similar to Scala collection methods. Two important differences are as follows:\n\n- Scala collection methods operate on data that can fit in the memory of a single machine, whereas RDD methods can operate on data distributed across cluster of nodes. \n- RDD transformations are *lazy*, whereas Scala collection methods are *strict*.\n\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": false,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>Transformations</strong></h3>\n<p><em>A transformation method of an RDD creates a new RDD by performing a computation on the source RDD.</em></p>\n<p>RDD transformations are conceptually similar to Scala collection methods. Two important differences are as follows:</p>\n<ul>\n<li>Scala collection methods operate on data that can fit in the memory of a single machine, whereas RDD methods can operate on data distributed across cluster of nodes.</li>\n<li>RDD transformations are <em>lazy</em>, whereas Scala collection methods are <em>strict</em>.</li>\n</ul>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692296_1579787140",
      "id": "20160824-233321_1545252486",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86365"
    },
    {
      "text": "%md\n\n#### **filter**\n\nThe *filter* method is a higher-order method that takes a Boolean function as input and applies it to each element in the source RDD to create a new RDD. A Boolean function takes an input and returns true or false. The filter method returns a new RDD formed by selecting only those elements for which the input Boolean function returned true. Thus, the new RDD contains a subset of the elements in the original RDD.",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h4><strong>filter</strong></h4>\n<p>The <em>filter</em> method is a higher-order method that takes a Boolean function as input and applies it to each element in the source RDD to create a new RDD. A Boolean function takes an input and returns true or false. The filter method returns a new RDD formed by selecting only those elements for which the input Boolean function returned true. Thus, the new RDD contains a subset of the elements in the original RDD.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692298_-576151190",
      "id": "20160824-233642_33725018",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86366"
    },
    {
      "title": "filter() example",
      "text": "%spark\n\nval inputRDD = spark.sparkContext.textFile(\"../../../../Data/README.md\")\n\nval sparkRDD = inputRDD.filter(line => line.contains(\"spark\"))\n\nsparkRDD.collect.foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:50:49+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "inputRDD: org.apache.spark.rdd.RDD[String] = ../../../../Data/README.md MapPartitionsRDD[19] at textFile at <console>:23\nsparkRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at filter at <console>:25\n<http://spark.apache.org/>\nguide, on the [project web page](http://spark.apache.org/documentation.html)\n[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n    ./bin/spark-shell\n    ./bin/pyspark\nexamples to a cluster. This can be a mesos:// or spark:// URL,\n    MASTER=spark://host:7077 ./bin/run-example SparkPi\nTesting first requires [building Spark](#building-spark). Once Spark is built, tests\n[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nPlease refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.185:4041/jobs/job?id=4"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1545743692299_1214465442",
      "id": "20160822-112742_796804883",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "dateStarted": "2018-12-25T15:50:49+0200",
      "dateFinished": "2018-12-25T15:50:50+0200",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86367"
    },
    {
      "text": "%md\n\n### **map**\n\nThe *map* method is a higher-order method that takes a function as input and applies it to each element in the source RDD to create a new RDD. The input function to map must take a single input parameter and return a value.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>map</strong></h3>\n<p>The <em>map</em> method is a higher-order method that takes a function as input and applies it to each element in the source RDD to create a new RDD. The input function to map must take a single input parameter and return a value.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692300_-1829581620",
      "id": "20160824-233804_2111981647",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86368"
    },
    {
      "title": "map() example",
      "text": "%spark\n\nval inputRDD = spark.sparkContext.textFile(\"../../../../Data/README.md\")\n\nval lcRDD = inputRDD.map { line => line.trim.toLowerCase() }\n\nlcRDD.take(10).foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:51:01+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "inputRDD: org.apache.spark.rdd.RDD[String] = ../../../../Data/README.md MapPartitionsRDD[22] at textFile at <console>:23\nlcRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[23] at map at <console>:25\n# apache spark\n\nspark is a fast and general cluster computing system for big data. it provides\nhigh-level apis in scala, java, python, and r, and an optimized engine that\nsupports general computation graphs for data analysis. it also supports a\nrich set of higher-level tools including spark sql for sql and dataframes,\nmllib for machine learning, graphx for graph processing,\nand spark streaming for stream processing.\n\n<http://spark.apache.org/>\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.185:4041/jobs/job?id=5"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1545743692303_545936678",
      "id": "20160822-113151_205163671",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "dateStarted": "2018-12-25T15:51:01+0200",
      "dateFinished": "2018-12-25T15:51:03+0200",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86369"
    },
    {
      "text": "%md\n\n### **flatMap**\n\nThe *flatMap* method is a higher-order method that takes an input function, which returns a sequence for each input element passed to it. The flatMap method returns a new RDD formed by flattening this collection of sequence.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>flatMap</strong></h3>\n<p>The <em>flatMap</em> method is a higher-order method that takes an input function, which returns a sequence for each input element passed to it. The flatMap method returns a new RDD formed by flattening this collection of sequence.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692305_400237372",
      "id": "20160824-233849_561794074",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86370"
    },
    {
      "title": "flatMap() example",
      "text": "%spark\n\nval lines = spark.sparkContext.textFile(\"../../../../Data/insightedge.txt\")\n\nval words = lines.flatMap { l => l.split(\" \") }\n\nwords.collect.foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:54:13+0200",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "lines: org.apache.spark.rdd.RDD[String] = ../../../../Data/insightedge.txt MapPartitionsRDD[28] at textFile at <console>:23\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[29] at flatMap at <console>:25\nInsightEdge\nis\na\nhigh\nperformance\nSpark\ndistribution\nfor\nlow\nlatency\nworkloads,\nhigh\navailability,\nand\nhybrid\ntransactional/analytical\nprocessing\nin\none\nunified\nsolution.\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.185:4041/jobs/job?id=6"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1545743692317_349033183",
      "id": "20160824-233922_431674084",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "dateStarted": "2018-12-25T15:54:14+0200",
      "dateFinished": "2018-12-25T15:54:15+0200",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86371"
    },
    {
      "text": "%md\n\n### **zip**\n\nThe *zip* method takes an RDD as input and returns an RDD of pairs, where the first element in a pair is from the source RDD and second element is from the input RDD. Unlike the cartesian method, the RDD returned by zip has the same number of elements as the source RDD. Both the source RDD and the input RDD must have the same length. In addition, both RDDs are assumed to have same number of partitions and same number of elements in each partition.",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>zip</strong></h3>\n<p>The <em>zip</em> method takes an RDD as input and returns an RDD of pairs, where the first element in a pair is from the source RDD and second element is from the input RDD. Unlike the cartesian method, the RDD returned by zip has the same number of elements as the source RDD. Both the source RDD and the input RDD must have the same length. In addition, both RDDs are assumed to have same number of partitions and same number of elements in each partition.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692319_-139142703",
      "id": "20160824-234712_2050007032",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86372"
    },
    {
      "title": "zip() example",
      "text": "%spark\n\nval numbers = spark.sparkContext.parallelize(List(1, 2, 3, 4))\n\nval alphabets = spark.sparkContext.parallelize(List(\"a\", \"b\", \"c\", \"d\"))\n\nval zippedPairs = numbers.zip(alphabets)\n\nzippedPairs.collect.foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:54:20+0200",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "numbers: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[30] at parallelize at <console>:23\nalphabets: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[31] at parallelize at <console>:23\nzippedPairs: org.apache.spark.rdd.RDD[(Int, String)] = ZippedPartitionsRDD2[32] at zip at <console>:27\n(1,a)\n(2,b)\n(3,c)\n(4,d)\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.185:4041/jobs/job?id=7"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1545743692322_2090986465",
      "id": "20160824-232917_935402122",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "dateStarted": "2018-12-25T15:54:20+0200",
      "dateFinished": "2018-12-25T15:54:23+0200",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86373"
    },
    {
      "text": "%md\n\n### **zipWithIndex**\n\nThe *zipWithIndex* method zips the elements of the source RDD with their indices and returns an RDD of pairs.",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>zipWithIndex</strong></h3>\n<p>The <em>zipWithIndex</em> method zips the elements of the source RDD with their indices and returns an RDD of pairs.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692324_1309579292",
      "id": "20160824-235459_880644048",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86374"
    },
    {
      "title": "zipWithIndex() example",
      "text": "%spark\n\nval alphabets = spark.sparkContext.parallelize(List(\"a\", \"b\", \"c\", \"d\"))\n\nval alphabetsWithIndex = alphabets.zipWithIndex\n\nalphabetsWithIndex.collect.foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692327_2013119149",
      "id": "20160824-235531_65637691",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86375"
    },
    {
      "text": "%md\n\n## **Actions**\n\n*Actions are RDD methods that return a value to a driver program.*",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h2><strong>Actions</strong></h2>\n<p><em>Actions are RDD methods that return a value to a driver program.</em></p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692329_-1903668023",
      "id": "20160825-000239_56061675",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86376"
    },
    {
      "text": "%md\n\n### **collect**\n\nThe *collect* method returns the elements in the source RDD as an array.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>collect</strong></h3>\n<p>The <em>collect</em> method returns the elements in the source RDD as an array.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692332_-1653194228",
      "id": "20160825-000713_1738668419",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86377"
    },
    {
      "title": "collect() action example",
      "text": "%spark\n\nval rdd = spark.sparkContext.parallelize((1 to 10000).toList)\n\nval filteredRdd = rdd filter { x => (x % 1000) == 0 }\n\nval filterResult = filteredRdd.collect\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692333_-496617272",
      "id": "20160825-000853_584129715",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86378"
    },
    {
      "text": "%md\n\n### **count**\n\nThe *count* method returns a count of the elements in the source RDD.",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>count</strong></h3>\n<p>The <em>count</em> method returns a count of the elements in the source RDD.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692334_-1681206601",
      "id": "20160825-000943_1916075260",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86379"
    },
    {
      "title": "count() action example",
      "text": "%spark\n\nval xs = (1 to 100).toList\nval xsRdd = spark.sparkContext.parallelize(xs)\nval evenRdd = xsRdd.filter { _ % 2 == 0}\nval count = evenRdd.count\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692336_996238079",
      "id": "20160822-113806_1236036492",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86380"
    },
    {
      "text": "%md\n\n### **first**\n\nThe *first* method returns the first element in the source RDD.",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>first</strong></h3>\n<p>The <em>first</em> method returns the first element in the source RDD.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692337_114221199",
      "id": "20160825-001125_1832272884",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86381"
    },
    {
      "title": "first() action example",
      "text": "%spark\n\nval xs = (1 to 100).toList\nval xsRdd = spark.sparkContext.parallelize(xs)\nval evenRdd = xsRdd.filter { _ % 2 == 0}\nval firstEven = evenRdd.first()\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692338_-1767470116",
      "id": "20160822-114007_1394756535",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86382"
    },
    {
      "text": "%md\n\n### **take**\n\nThe *take* method takes an integer N as input and returns an array containing the first N element in the source RDD.",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>take</strong></h3>\n<p>The <em>take</em> method takes an integer N as input and returns an array containing the first N element in the source RDD.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692340_170149155",
      "id": "20160825-001230_767406766",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86383"
    },
    {
      "title": "take() action example",
      "text": "%spark\n\nval xs = (1 to 100).toList\nval xsRdd = spark.sparkContext.parallelize(xs)\nval evenRdd = xsRdd.filter { _ % 2 == 0}\nval first5 = evenRdd.take(5)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692341_-1597877476",
      "id": "20160822-114155_1726504292",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86384"
    },
    {
      "text": "%md\n\n### **fold**\n\nThe higher-order *fold* method aggregates the elements in the source RDD using the specified neutral zero value and an associative binary operator. It first aggregates the elements in each RDD partition and then aggregates the results from each partition.\nThe neutral zero value depends on the RDD type and the aggregation operation. For example, if we want to sum all the elements in an RDD of Integers, the neutral zero value should be 0. Instead, if we want to calculate the products of all the elements in an RDD of Integers, the neutral zero value should be 1.\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": { },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>fold</strong></h3>\n<p>The higher-order <em>fold</em> method aggregates the elements in the source RDD using the specified neutral zero value and an associative binary operator. It first aggregates the elements in each RDD partition and then aggregates the results from each partition.\n<br  />The neutral zero value depends on the RDD type and the aggregation operation. For example, if we want to sum all the elements in an RDD of Integers, the neutral zero value should be 0. Instead, if we want to calculate the products of all the elements in an RDD of Integers, the neutral zero value should be 1.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692343_734682791",
      "id": "20160824-232851_2092442258",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86385"
    },
    {
      "title": "fold() action example",
      "text": "%spark\n\nval numbersRdd = spark.sparkContext.parallelize(List(2, 5, 3, 1))\n\nval sum = numbersRdd.fold(0) ((s, x) => s + x)\n\nval product = numbersRdd.fold(1) ((p, x) => p * x)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692344_-300894727",
      "id": "20160825-001657_1514729699",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86386"
    },
    {
      "text": "%md\n\n### **reduce**\n\nThe higher-order **_reduce_** method aggregates the elements of the source RDD using **an associative and commutative binary operator** provided to it. It is similar to the fold method; however, it does not require a neutral zero value.",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": { }
            }
          }
        ],
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<h3><strong>reduce</strong></h3>\n<p>The higher-order <strong><em>reduce</em></strong> method aggregates the elements of the source RDD using <strong>an associative and commutative binary operator</strong> provided to it. It is similar to the fold method; however, it does not require a neutral zero value.</p>\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1545743692345_348804865",
      "id": "20160825-001841_85058628",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86387"
    },
    {
      "text": "%spark\n\nval numbersRdd = spark.sparkContext.parallelize(List(2, 5, 3, 1))\n\nval sum = numbersRdd.reduce ((x, y) => x + y)\n\nval product = numbersRdd.reduce((x, y) => x * y)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692347_1802371140",
      "id": "20160825-001933_1071665094",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86388"
    },
    {
      "title": "Scala Word Count in README.md file, Ten Most Common Words",
      "text": "%spark\n\nval readmeFile = spark.sparkContext.textFile(\"../../../../Data/README.md\") \n\nval counts = readmeFile.flatMap(line => line.split(\" \")). map(word => (word, 1)). reduceByKey(_ + _) \n\nval sortedCounts = counts.sortBy(_._2, false) \n\nsortedCounts.take(10).foreach(println)\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:55:33+0200",
      "config": {
        "lineNumbers": false,
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "readmeFile: org.apache.spark.rdd.RDD[String] = ../../../../Data/README.md MapPartitionsRDD[34] at textFile at <console>:23\ncounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[37] at reduceByKey at <console>:25\nsortedCounts: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[42] at sortBy at <console>:27\n(,67)\n(the,21)\n(to,14)\n(Spark,13)\n(for,11)\n(and,10)\n(##,8)\n(a,8)\n(run,7)\n(can,6)\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://192.168.9.185:4041/jobs/job?id=8",
            "http://192.168.9.185:4041/jobs/job?id=9"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1545743692353_952697647",
      "id": "20160822-114941_856193120",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "dateStarted": "2018-12-25T15:55:33+0200",
      "dateFinished": "2018-12-25T15:55:36+0200",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86389"
    },
    {
      "user": "anonymous",
      "dateUpdated": "2018-12-25T15:14:52+0200",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "results": { },
        "graph": {
          "mode": "table",
          "height": 300,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": { }
        },
        "enabled": true,
        "fontSize": 9
      },
      "settings": {
        "params": { },
        "forms": { }
      },
      "apps": [],
      "jobName": "paragraph_1545743692354_578126301",
      "id": "20160919-110818_1918993395",
      "dateCreated": "2018-12-25T15:14:52+0200",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500,
      "$$hashKey": "object:86390"
    }
  ],
  "name": "Lab 5.1 Examples - RDD Transformations and Actions",
  "id": "2DY5HAJD4",
  "noteParams": { },
  "noteForms": { },
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": { }
}