{"paragraphs":[{"title":"Getting information about a Dataframe","text":"%spark\n\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n\nval dataSchema = new StructType(Array(\n    new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n    new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n    new StructField(\"count\", LongType, false)\n))\n\nval flightDf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(dataSchema).load(\"../../../../Data/2010-summary.csv\")\n    \nflightDf.cache\n\nflightDf.columns\n\nflightDf.explain\n\nflightDf.dtypes\n\nflightDf.printSchema\n\nflightDf.show\n","user":"anonymous","dateUpdated":"2018-12-25T17:21:15+0200","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\ndataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\nflightDf: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres75: flightDf.type = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres76: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)\n== Physical Plan ==\nInMemoryTableScan [DEST_COUNTRY_NAME#101, ORIGIN_COUNTRY_NAME#102, count#103L]\n   +- InMemoryRelation [DEST_COUNTRY_NAME#101, ORIGIN_COUNTRY_NAME#102, count#103L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- *(1) FileScan csv [DEST_COUNTRY_NAME#101,ORIGIN_COUNTRY_NAME#102,count#103L] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/yuval/XAP-Builds/Data/2010-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\nres78: Array[(String, String)] = Array((DEST_COUNTRY_NAME,StringType), (ORIGIN_COUNTRY_NAME,StringType), (count,LongType))\nroot\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4041/jobs/job?id=35"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1545751042936_-33357854","id":"20160822-145305_1775455394","dateCreated":"2018-12-25T17:17:22+0200","dateStarted":"2018-12-25T17:21:15+0200","dateFinished":"2018-12-25T17:21:18+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:49573"},{"text":"%md\n\n## Language-integrated query methods of the DataFrame class\n\nWe will create some dataframes programmatically to illustrate the language integrated query methods.","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Language-integrated query methods of the DataFrame class</h2>\n<p>We will create some dataframes programmatically to illustrate the language integrated query methods.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545751042941_-652970760","id":"20160823-174226_141584309","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49574"},{"text":"%spark\n\ncase class Customer(cId: Long, name: String, age: Int, gender: String)\nval customers = List(\n    Customer(1, \"James\", 21, \"M\"),\n    Customer(2, \"Liz\", 25, \"F\"),\n    Customer(3, \"John\", 31, \"M\"),\n    Customer(4, \"Jennifer\", 45, \"F\"),\n    Customer(5, \"Robert\", 41, \"M\"),\n    Customer(6, \"Sandra\", 45, \"F\")\n)\nval customerDF = sc.parallelize(customers).toDF()\n\ncase class Product(pId: Long, name: String, price: Double, cost: Double)\n\nval products = List(\n    Product(1, \"Dell\", 600, 400),\n    Product(2, \"Kindle\", 100, 40),\n    Product(3, \"iPad\", 600, 500),\n    Product(4, \"Galaxy\", 600, 400),\n    Product(5, \"MacBook\", 1200, 900),\n    Product(6, \"iPhone\", 500, 400)\n)\nval productDF = sc.parallelize(products).toDF()\n\ncase class SalesSummary(date: String, product: String, country: String, revenue: Double)\n\nval sales = List(SalesSummary(\"01/01/2015\", \"Chromebook\", \"USA\", 40000),\nSalesSummary(\"01/02/2015\", \"Chromebook\", \"USA\", 30000),\nSalesSummary(\"01/01/2015\", \"Chromebook\", \"India\", 10000),\nSalesSummary(\"01/02/2015\", \"Chromebook\", \"India\", 5000),\nSalesSummary(\"01/01/2015\", \"Macbook\", \"USA\", 20000),\nSalesSummary(\"01/02/2015\", \"Macbook\", \"USA\", 10000),\nSalesSummary(\"01/01/2015\", \"Macbook\", \"India\", 9000),\nSalesSummary(\"01/02/2015\", \"Macbook\", \"India\", 6000))\n\nval salesDF = sc.parallelize(sales).toDF()","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Customer\ncustomers: List[Customer] = List(Customer(1,James,21,M), Customer(2,Liz,25,F), Customer(3,John,31,M), Customer(4,Jennifer,45,F), Customer(5,Robert,41,M), Customer(6,Sandra,45,F))\ncustomerDF: org.apache.spark.sql.DataFrame = [cId: bigint, name: string ... 2 more fields]\ndefined class Product\nproducts: List[Product] = List(Product(1,Dell,600.0,400.0), Product(2,Kindle,100.0,40.0), Product(3,iPad,600.0,500.0), Product(4,Galaxy,600.0,400.0), Product(5,MacBook,1200.0,900.0), Product(6,iPhone,500.0,400.0))\nproductDF: org.apache.spark.sql.DataFrame = [pId: bigint, name: string ... 2 more fields]\ndefined class SalesSummary\nsales: List[SalesSummary] = List(SalesSummary(01/01/2015,Chromebook,USA,40000.0), SalesSummary(01/02/2015,Chromebook,USA,30000.0), SalesSummary(01/01/2015,Chromebook,India,10000.0), SalesSummary(01/02/2015,Chromebook,India,5000.0), SalesSummary(01/01/2015,Macbook,USA,20000.0), SalesSummary(01/02/2015,Macbook,USA,10000.0), SalesSummary(01/01/2015,Macbook,India,9000.0), SalesSummary(01/02/2015,Macbook,India,6000.0))\nsalesDF: org.apache.spark.sql.DataFrame = [date: string, product: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1545751042941_-558937362","id":"20160823-175111_1341992873","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49575"},{"text":"%md\n\n## agg\n\nThe **agg** method performs specified aggregations on one or more columns in the source DataFrame and\nreturns the result as a new DataFrame.","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>agg</h2>\n<p>The <strong>agg</strong> method performs specified aggregations on one or more columns in the source DataFrame and<br/>returns the result as a new DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545751042942_-375238936","id":"20160823-175710_728578499","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49576"},{"text":"%spark\n\nval aggregates = productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\n\naggregates.show\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aggregates: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1545751042942_628087602","id":"20160823-180618_860695558","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49577"},{"text":"%md\n\n## apply\n\nThe **apply** method takes the name of a column as an argument and returns the specified column in the\nsource DataFrame as an instance of the **Column** class. The **Column** class provides operators for manipulating a\ncolumn in a DataFrame.\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>apply</h2>\n<p>The <strong>apply</strong> method takes the name of a column as an argument and returns the specified column in the<br/>source DataFrame as an instance of the <strong>Column</strong> class. The <strong>Column</strong> class provides operators for manipulating a<br/>column in a DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545751042942_151043202","id":"20160823-180643_1652513120","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49578"},{"text":"%spark\n\nval priceColumn = productDF.apply(\"price\")\n\nval discountedPriceColumn = priceColumn * 0.10","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"priceColumn: org.apache.spark.sql.Column = price\ndiscountedPriceColumn: org.apache.spark.sql.Column = (price * 0.1)\n"}]},"apps":[],"jobName":"paragraph_1545751042943_2104443845","id":"20160823-180831_1959446161","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49579"},{"text":"%md\n\nScala provides syntactic sugar that allows you to use productDF(\"price\") instead of productDF.apply(\"price\"). \n\nIt automatically converts productDF(\"price\") to productDF.apply(\"price\"). \n\nSo the preceding code can be rewritten as follows:","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Scala provides syntactic sugar that allows you to use productDF(&ldquo;price&rdquo;) instead of productDF.apply(&ldquo;price&rdquo;).</p>\n<p>It automatically converts productDF(&ldquo;price&rdquo;) to productDF.apply(&ldquo;price&rdquo;).</p>\n<p>So the preceding code can be rewritten as follows:</p>\n"}]},"apps":[],"jobName":"paragraph_1545751042943_1203854429","id":"20160823-181039_165411325","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49580"},{"text":"%spark\n\nval priceColumn = productDF(\"price\")\n\nval discountedPriceColumn = priceColumn * 0.5","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"priceColumn: org.apache.spark.sql.Column = price\ndiscountedPriceColumn: org.apache.spark.sql.Column = (price * 0.5)\n"}]},"apps":[],"jobName":"paragraph_1545751042943_878503422","id":"20160823-181124_19808600","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49581"},{"text":"%md\n\n## Notes on the **Column** class\n\nAn instance of the **Column** class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library.\n\nIf a method or function expects an instance of the Column class as an argument, we can use the **$\"...\"** notation to select a column in a DataFrame.\n\nFor example, the following three aggregates are equivalent:\n\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Notes on the <strong>Column</strong> class</h2>\n<p>An instance of the <strong>Column</strong> class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library.</p>\n<p>If a method or function expects an instance of the Column class as an argument, we can use the <strong>$&ldquo;&hellip;&ldquo;</strong> notation to select a column in a DataFrame.</p>\n<p>For example, the following three aggregates are equivalent:</p>\n"}]},"apps":[],"jobName":"paragraph_1545751042943_1561834779","id":"20160823-181235_559377109","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49582"},{"text":"%spark\n\nval aggregates1 = productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")), count(productDF(\"name\")))\n\naggregates1.show\n\nval aggregates2 = productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\n\naggregates2.show\n\nval aggregates3 = productDF.agg(max($\"price\"), min($\"price\"), count($\"name\"))\n\naggregates3.show\n\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aggregates1: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\naggregates2: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\naggregates3: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1545751042944_-569065685","id":"20160823-181903_1519059300","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49583"},{"text":"%md\n\n### **select**\n\nThe *select* method returns a DataFrame containing only the specified columns from the source DataFrame.\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>select</strong></h3>\n<p>The <em>select</em> method returns a DataFrame containing only the specified columns from the source DataFrame.</p>\n"}]},"apps":[],"jobName":"paragraph_1545751042944_-1109850302","id":"20160825-004227_2034053977","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49584"},{"text":"%spark\n\nval namesAgeDF = customerDF.select(\"name\", \"age\")\n\nnamesAgeDF.show","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"namesAgeDF: org.apache.spark.sql.DataFrame = [name: string, age: int]\n+--------+---+\n|    name|age|\n+--------+---+\n|   James| 21|\n|     Liz| 25|\n|    John| 31|\n|Jennifer| 45|\n|  Robert| 41|\n|  Sandra| 45|\n+--------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1545751042944_1615169101","id":"20160825-004303_1682213357","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49585"},{"text":"%md\n\n### **filter**\n\nThe filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>filter</strong></h3>\n<p>The filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.</p>\n"}]},"apps":[],"jobName":"paragraph_1545751042945_1173486917","id":"20160823-182045_153856142","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49586"},{"text":"%spark\n\nval filteredDF = customerDF.filter(\"age > 25\")\n\nfilteredDF.show\n\nval filteredDF2 = customerDF.filter($\"age\" > 40)\n\nfilteredDF2.show\n\nval filteredDF3 = customerDF.filter(customerDF(\"age\") > 20)\n\nfilteredDF3.show\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filteredDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cId: bigint, name: string ... 2 more fields]\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\nfilteredDF2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cId: bigint, name: string ... 2 more fields]\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\nfilteredDF3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cId: bigint, name: string ... 2 more fields]\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  2|     Liz| 25|     F|\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\n"}]},"apps":[],"jobName":"paragraph_1545751042945_1258178071","id":"20160825-002529_441878203","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49587"},{"text":"%md\n\n### **groupBy**\n\nThe *groupBy* method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>groupBy</strong></h3>\n<p>The <em>groupBy</em> method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.</p>\n"}]},"apps":[],"jobName":"paragraph_1545751042945_-1819078414","id":"20160825-003138_1418319608","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49588"},{"text":"%spark\n\nval countByGender = customerDF.groupBy(\"gender\").count\n\ncountByGender.show\n\nval revenueByProductDF = salesDF.groupBy(\"product\").sum(\"revenue\")\n\nrevenueByProductDF.show\n","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545751042946_-1238411522","id":"20160825-002634_332193277","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49589"},{"text":"%md\n\n### **orderBy**\n\nThe *orderBy* method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>orderBy</strong></h3>\n<p>The <em>orderBy</em> method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.</p>\n"}]},"apps":[],"jobName":"paragraph_1545751042946_1269494477","id":"20160825-003347_1250302752","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49590"},{"text":"%spark\n\nval sortedDF = customerDF.orderBy(\"name\")\n\nsortedDF.show","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545751042947_1224523987","id":"20160825-004007_244050346","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49591"},{"text":"","user":"anonymous","dateUpdated":"2018-12-25T17:17:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545751042947_-1436565772","id":"20160825-005916_631693256","dateCreated":"2018-12-25T17:17:22+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49592"}],"name":"Lab 6.3 Solution - DataFrame Operations","id":"2E1MA46VD","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}