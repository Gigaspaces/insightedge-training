{"paragraphs":[{"text":"%md\n\n## Using Parquet files with Spark\n\nParquet is an interoperable columnar storage format. It focusses on space efficiency and query efficiency. Parquet's origin is based on Google's Dremel and was developed by Twitter and Cloudera.\n\nParquet is now an Apache project - [Apache Parquet](https://parquet.apache.org).\n\nSpark can load data from Parquet files and save data as Parquet files.\n\n","user":"anonymous","dateUpdated":"2018-12-31T15:07:55+0200","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Using Parquet files with Spark</h2>\n<p>Parquet is an interoperable columnar storage format. It focusses on space efficiency and query efficiency. Parquet's origin is based on Google's Dremel and was developed by Twitter and Cloudera.</p>\n<p>Parquet is now an Apache project - <a href=\"https://parquet.apache.org\">Apache Parquet</a>.</p>\n<p>Spark can load data from Parquet files and save data as Parquet files.</p>\n"}]},"apps":[],"jobName":"paragraph_1546261675197_537522615","id":"20160822-165125_1131574017","dateCreated":"2018-12-31T15:07:55+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9027"},{"title":"Saving data as Parquet file","text":"%spark\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport sqlContext.implicits._\n\ncase class Customer(age: Int, job: String, balance: Int)\n\nval rowsRDD = sc.textFile(\"../../../../data/bank.csv\").filter(! _.startsWith(\"\\\"age\\\"\"))\n\nval customerRDD = rowsRDD.map{row => row.split(\";\")}.map{cols => Customer(cols(0).trim.toInt, cols(1), cols(5).trim.toInt)}\n\nval customerDF = customerRDD.toDF()\n\ncustomerDF.write.format(\"parquet\").save(\"/tmp/bank.parquet\")\n","user":"anonymous","dateUpdated":"2018-12-31T15:26:50+0200","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark._\nimport org.apache.spark.sql._\nimport sqlContext.implicits._\ndefined class Customer\nrowsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[58] at filter at <console>:78\ncustomerRDD: org.apache.spark.rdd.RDD[Customer] = MapPartitionsRDD[60] at map at <console>:82\ncustomerDF: org.apache.spark.sql.DataFrame = [age: int, job: string ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4042/jobs/job?id=4"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1546261675200_1424318184","id":"20160823-111517_1662906000","dateCreated":"2018-12-31T15:07:55+0200","dateStarted":"2018-12-31T15:25:30+0200","dateFinished":"2018-12-31T15:25:41+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9028"},{"title":"Loading data from Parquet file","text":"%spark\n\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\nval customerDfFromParquet = sqlContext.load(\"/tmp/bank.parquet\")\n\ncustomerDfFromParquet.printSchema\n\ncustomerDfFromParquet.show\n","user":"anonymous","dateUpdated":"2018-12-31T15:27:12+0200","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@2002c755\nwarning: there was one deprecation warning; re-run with -deprecation for details\ncustomerDfFromParquet: org.apache.spark.sql.DataFrame = [age: int, job: string ... 1 more field]\nroot\n |-- age: integer (nullable = true)\n |-- job: string (nullable = true)\n |-- balance: integer (nullable = true)\n\n+---+---------------+-------+\n|age|            job|balance|\n+---+---------------+-------+\n| 30|   \"unemployed\"|   1787|\n| 33|     \"services\"|   4789|\n| 35|   \"management\"|   1350|\n| 30|   \"management\"|   1476|\n| 59|  \"blue-collar\"|      0|\n| 35|   \"management\"|    747|\n| 36|\"self-employed\"|    307|\n| 39|   \"technician\"|    147|\n| 41| \"entrepreneur\"|    221|\n| 43|     \"services\"|    -88|\n| 39|     \"services\"|   9374|\n| 43|       \"admin.\"|    264|\n| 36|   \"technician\"|   1109|\n| 20|      \"student\"|    502|\n| 31|  \"blue-collar\"|    360|\n| 40|   \"management\"|    194|\n| 56|   \"technician\"|   4073|\n| 37|       \"admin.\"|   2317|\n| 25|  \"blue-collar\"|   -221|\n| 31|     \"services\"|    132|\n+---+---------------+-------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4042/jobs/job?id=5","http://192.168.9.185:4042/jobs/job?id=6"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1546261675201_-1152781322","id":"20160823-112553_610561150","dateCreated":"2018-12-31T15:07:55+0200","dateStarted":"2018-12-31T15:27:12+0200","dateFinished":"2018-12-31T15:27:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9029"},{"text":"%md\n\n## Using JSON files with Spark\n\nSpark can read data from JSON files.\n","user":"anonymous","dateUpdated":"2018-12-31T15:07:55+0200","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Using JSON files with Spark</h2>\n<p>Spark can read data from JSON files.</p>\n"}]},"apps":[],"jobName":"paragraph_1546261675204_653367975","id":"20160823-120136_710583192","dateCreated":"2018-12-31T15:07:55+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9030"},{"title":"Loading data from JSON","text":"%spark\n\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\nimport sqlContext.implicits._\n\nval personDF: DataFrame = sqlContext.jsonFile(\"../../../../data/persons.json\")\n\npersonDF.show\n","user":"anonymous","dateUpdated":"2018-12-31T15:31:20+0200","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@ecdf8fb\nimport sqlContext.implicits._\nwarning: there was one deprecation warning; re-run with -deprecation for details\npersonDF: org.apache.spark.sql.DataFrame = [age: bigint, first_name: string ... 1 more field]\n+---+----------+---------+\n|age|first_name|last_name|\n+---+----------+---------+\n| 53|       Jim|    Smith|\n| 68|      Paul|     Ross|\n| 68|   Brandon|    Cliff|\n+---+----------+---------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4042/jobs/job?id=7","http://192.168.9.185:4042/jobs/job?id=8"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1546261675208_-80072547","id":"20160823-120614_1736879537","dateCreated":"2018-12-31T15:07:55+0200","dateStarted":"2018-12-31T15:31:20+0200","dateFinished":"2018-12-31T15:31:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9031"},{"title":"Saving data as JSON file","text":"%spark\n\npersonDF.select(\"first_name\", \"last_name\").write.json(\"/tmp/personNames.json\")\n","user":"anonymous","dateUpdated":"2018-12-31T15:31:50+0200","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4042/jobs/job?id=9"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1546261675210_781849400","id":"20160823-120800_314999582","dateCreated":"2018-12-31T15:07:55+0200","dateStarted":"2018-12-31T15:31:50+0200","dateFinished":"2018-12-31T15:31:52+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9032"},{"title":"Reading data from JSON file whose schema is known","text":"%spark\n\nimport org.apache.spark.sql.types._\n\nval personSchema = StructType(List(\n        StructField(\"first_name\", StringType, false),\n        StructField(\"last_name\", StringType, false),\n        StructField(\"age\", IntegerType, false)\n    ))\n    \nval personDF = sqlContext.read.schema(personSchema).json(\"../../../../data/persons.json\")\n\npersonDF.printSchema\n\npersonDF.show","user":"anonymous","dateUpdated":"2018-12-31T15:32:26+0200","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\npersonSchema: org.apache.spark.sql.types.StructType = StructType(StructField(first_name,StringType,false), StructField(last_name,StringType,false), StructField(age,IntegerType,false))\npersonDF: org.apache.spark.sql.DataFrame = [first_name: string, last_name: string ... 1 more field]\nroot\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- age: integer (nullable = true)\n\n+----------+---------+---+\n|first_name|last_name|age|\n+----------+---------+---+\n|       Jim|    Smith| 53|\n|      Paul|     Ross| 68|\n|   Brandon|    Cliff| 68|\n+----------+---------+---+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4042/jobs/job?id=10"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1546261675212_464063398","id":"20160823-122854_1466425181","dateCreated":"2018-12-31T15:07:55+0200","dateStarted":"2018-12-31T15:32:26+0200","dateFinished":"2018-12-31T15:32:31+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9033"}],"name":"Lab 6.2 Solution - DataFrame Loading and Saving Data","id":"2DZJ8U49M","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}