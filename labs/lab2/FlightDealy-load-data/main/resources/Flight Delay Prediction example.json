{"paragraphs":[{"text":"%md\n> If you see 'Interpreter binding' above, just hit `Save` without deselecting any interpreters.\n\n## Welcome to Flights example.\n\n##### This example shows some features of InsightEdge:\n\n* 1.Define flight records that are related to flight number and date(see java code,but can be done here aswell) \n* 2.Read csv file containing actual flight details with delays. \n* 3.Possible to Enrich the delay records with weather data of the same date at flight origin and destination and if its holiday etc. \n* 4.Save data to the grid\n* 5.Train machine learning  algo with grid data so it will be able to predict if there will be delay(todo test + improve the model ) \n* 6.Save the model to the grid \n* 7.Load the model from the grid and predict flight delays for new data(possible to add  weather forecast for related date)\n* 8.Inform the grid on predicted delays\n* 9.Grid will use delay objects to trigger logic\n\n##### This is a live tutorial, you can run the code yourself. _(click `Run` button in each paragraph from top to bottom)_","user":"anonymous","dateUpdated":"2018-01-09T09:17:48+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<blockquote><p>If you see 'Interpreter binding' above, just hit <code>Save</code> without deselecting any interpreters.</p>\n</blockquote>\n<h2>Welcome to Flights example.</h2>\n<h5>This example shows some features of InsightEdge:</h5>\n<ul>\n<li>1.Define flight records that are related to flight number and date(see java code,but can be done here aswell)</li>\n<li>2.Read csv file containing actual flight details with delays.</li>\n<li>3.Possible to Enrich the delay records with weather data of the same date at flight origin and destination and if its holiday etc.</li>\n<li>4.Save data to the grid</li>\n<li>5.Train machine learning  algo with grid data so it will be able to predict if there will be delay( test + improve the model )</li>\n<li>6.Save the model to the grid</li>\n<li>7.Load the model from the grid and predict flight delays for new data(possible to add  weather forecast for related date)</li>\n<li>8.Inform the grid on predicted delays</li>\n<li>9.Grid will use delay objects to trigger logic</li>\n</ul>\n<h5>This is a live tutorial, you can run the code yourself. <em>(click <code>Run</code> button in each paragraph from top to bottom)</em></h5>\n"}]},"apps":[],"jobName":"paragraph_1514725957296_-597929635","id":"20170620-093024_406952967","dateCreated":"2017-12-31T15:12:37+0200","dateStarted":"2018-01-08T16:17:38+0200","dateFinished":"2018-01-08T16:17:40+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:281"},{"title":"Import dependencies","text":"%dep\n\nz.reset()\nz.load(\"C:/gs/12.3/gigaspaces-insightedge-12.3.0-m10-b18910/gigaspaces-insightedge-12.3.0-m10-b18910/insightedge/examples/jars/FlightDelayDemo-1.0.0-SNAPSHOT.jar\")","user":"anonymous","dateUpdated":"2018-01-10T11:35:37+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@29030b03\n"}]},"apps":[],"jobName":"paragraph_1514968913251_776639925","id":"20180103-104153_1795376572","dateCreated":"2018-01-03T10:41:53+0200","dateStarted":"2018-01-10T11:35:39+0200","dateFinished":"2018-01-10T11:36:10+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:282"},{"text":"%md\n## Defining data model.\n\n##### `%define` allows you to append custom code to spark jobs. This is a nice place to write your model classes. To learn about syntax, please, refer to [Data Modeling](http://insightedge.io/docs/010/2_modeling.html).\n##### In this case the class is defined in grid side and loaded from the dependencies jar, but it can be defined here aswell.\n##### In the next section we will read csv file containing flight delay data, and save it to the grid\n##### Data was taken from http://stat-computing.org/dataexpo/2009/the-data.html","user":"anonymous","dateUpdated":"2018-01-08T17:37:50+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Defining data model.</h2>\n<h5><code>%define</code> allows you to append custom code to spark jobs. This is a nice place to write your model classes. To learn about syntax, please, refer to <a href=\"http://insightedge.io/docs/010/2_modeling.html\">Data Modeling</a>.</h5>\n<h5>In this case the class is defined in grid side and loaded from the dependencies jar, but it can be defined here aswell.</h5>\n<h5>In the next section we will read csv file containing flight delay data, and save it to the grid</h5>\n<h5>Data was taken from http://stat-computing.org/dataexpo/2009/the-data.html</h5>\n"}]},"apps":[],"jobName":"paragraph_1514725957296_-597929635","id":"20170620-093134_1087314216","dateCreated":"2017-12-31T15:12:37+0200","dateStarted":"2018-01-08T17:37:50+0200","dateFinished":"2018-01-08T17:38:00+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:283"},{"title":"Saving Flight delay RDD to the Grid after reading from csv","text":"%spark\n\nimport common.DelayRec\nimport org.insightedge.spark.implicits.all._\n\n//Reading and filtering the data\nval data = sc.textFile(\"C:/gs/12.3/2007.csv\").map(line => line.split(\",\"))\n.filter(rec => rec(4).toString() != \"NA\") //filter NA values\n.filter(rec => rec(0) !=\"Year\")           //Filer header\n.filter(rec => rec(21).toString() != \"1\")   //filter canceled flights\n.filter(rec => rec(16).toString() == \"ORD\") //filter origin to have less records in the example\n.map(rec => new DelayRec( rec(9).toString(),rec(0).toInt,rec(1).toInt,rec(2).toInt,rec(3).toInt,rec(5).toInt,rec(15).toInt,rec(16).toString(),rec(18).toDouble))\n   \n//see how the data looks like    \ndata.take(5).map(x => x.toString() ).foreach(println)\n\n//save the data to the grid (routing is flight number, id is flight+date)\ndata.saveToGrid()\n    \n\n","user":"anonymous","dateUpdated":"2018-01-10T11:35:52+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"lineNumbers":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport common.DelayRec\n\nimport org.insightedge.spark.implicits.all._\n\ndata: org.apache.spark.rdd.RDD[common.DelayRec] = MapPartitionsRDD[7] at map at <console>:44\ncommon.DelayRec@69813fd\r\ncommon.DelayRec@7d1bc25\r\ncommon.DelayRec@561b0343\r\ncommon.DelayRec@2730f0f\r\ncommon.DelayRec@44b2f347\r\n"}]},"apps":[],"jobName":"paragraph_1514725957297_-598314384","id":"20170620-093337_2060899833","dateCreated":"2017-12-31T15:12:37+0200","dateStarted":"2018-01-10T11:35:54+0200","dateFinished":"2018-01-10T11:37:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:284"},{"title":"Loading RDD from the grid ","text":"%spark\nval delayRecRdd = sc.gridRdd[DelayRec]()\nval count = delayRecRdd.count()\n","user":"anonymous","dateUpdated":"2018-01-10T11:43:15+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndelayRecRdd: org.insightedge.spark.rdd.InsightEdgeRDD[common.DelayRec] = InsightEdgeRDD[16] at RDD at InsightEdgeAbstractRDD.scala:36\n\ncount: Long = 343480\n"}]},"apps":[],"jobName":"paragraph_1514725957297_-598314384","id":"20170620-093424_256644097","dateCreated":"2017-12-31T15:12:37+0200","dateStarted":"2018-01-10T11:43:22+0200","dateFinished":"2018-01-10T11:43:30+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:285"},{"title":"Loading from the grid as DataFrame and register as view - in order to use sql in later paragraphs","text":"%spark\nval df = spark.read.grid[DelayRec]\ndf.printSchema()\ndf.createOrReplaceTempView(\"DelayRecView\")\nval notDelayed = df.filter(df(\"depDelay\") < 15).count()\n","user":"anonymous","dateUpdated":"2018-01-10T11:36:26+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true,"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [crsDepTime: int, date: string ... 10 more fields]\nroot\n |-- crsDepTime: integer (nullable = true)\n |-- date: string (nullable = true)\n |-- dayOfMonth: integer (nullable = true)\n |-- dayOfWeek: integer (nullable = true)\n |-- depDelay: integer (nullable = true)\n |-- distance: double (nullable = false)\n |-- flightNumber: string (nullable = true)\n |-- hour: integer (nullable = false)\n |-- id: string (nullable = true)\n |-- month: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- year: integer (nullable = true)\n\r\n\nnotDelayed: Long = 238659\n"}]},"apps":[],"jobName":"paragraph_1514725957298_-597160137","id":"20170620-093540_604441252","dateCreated":"2017-12-31T15:12:37+0200","dateStarted":"2018-01-10T11:36:26+0200","dateFinished":"2018-01-10T11:37:44+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:286"},{"title":"Using grid data for sql queries","text":"%sql\n\n\nselect dayofWeek, case when depDelay > 15 then 'delayed' else 'ok' end , count(1)\nfrom DelayRecView\ngroup by dayofweek , case when depDelay > 15 then 'delayed' else 'ok' end \n\n","user":"anonymous","dateUpdated":"2018-01-08T18:59:40+0200","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"dayofWeek","index":0,"aggr":"sum"}],"groups":[{"name":"CASE WHEN (depDelay > 15) THEN delayed ELSE ok END","index":1,"aggr":"sum"}],"values":[{"name":"count(1)","index":2,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"dayofWeek\tCASE WHEN (depDelay > 15) THEN delayed ELSE ok END\tcount(1)\n1\tok\t35120\n5\tok\t35102\n5\tdelayed\t15552\n6\tdelayed\t10524\n2\tdelayed\t14392\n6\tok\t33022\n1\tdelayed\t16239\n3\tok\t35280\n4\tdelayed\t15979\n4\tok\t34044\n3\tdelayed\t14665\n2\tok\t35304\n7\tok\t33904\n7\tdelayed\t14353\n"}]},"apps":[],"jobName":"paragraph_1514962623871_476252028","id":"20180103-085703_2003026011","dateCreated":"2018-01-03T08:57:03+0200","dateStarted":"2018-01-08T18:59:40+0200","dateFinished":"2018-01-08T18:59:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:287"},{"text":"%sql\r\n\r\nselect cast( cast(crsDepTime as int) / 100 as int) as hour,  case when depDelay > 15 then 'delayed' else 'ok' end as delay, count(1) as count\r\nfrom  DelayRecView \r\ngroup by  cast( cast(crsDepTime as int) / 100 as int),  case when depDelay > 15 then 'delayed' else 'ok' end\r\n","user":"anonymous","dateUpdated":"2018-01-08T19:00:10+0200","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"hour","index":0,"aggr":"sum"}],"groups":[{"name":"delay","index":1,"aggr":"sum"}],"values":[{"name":"count","index":2,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"hour\tdelay\tcount\n12\tok\t12408\n13\tok\t19753\n20\tdelayed\t10113\n10\tok\t16781\n19\tok\t12359\n15\tok\t14024\n15\tdelayed\t7419\n21\tok\t7717\n8\tok\t19840\n5\tok\t803\n12\tdelayed\t4380\n20\tok\t13305\n10\tdelayed\t5147\n5\tdelayed\t69\n9\tdelayed\t6141\n6\tdelayed\t1926\n6\tok\t15174\n16\tdelayed\t8268\n21\tdelayed\t5121\n13\tdelayed\t7536\n18\tdelayed\t8769\n22\tok\t1401\n16\tok\t14350\n22\tdelayed\t702\n9\tok\t22646\n11\tdelayed\t5057\n14\tok\t12734\n8\tdelayed\t3910\n7\tok\t18656\n11\tok\t14752\n18\tok\t11967\n14\tdelayed\t6163\n17\tok\t13106\n19\tdelayed\t9388\n7\tdelayed\t3188\n17\tdelayed\t8407\n"}]},"apps":[],"jobName":"paragraph_1514963887925_139603842","id":"20180103-091807_546038974","dateCreated":"2018-01-03T09:18:07+0200","dateStarted":"2018-01-08T19:00:10+0200","dateFinished":"2018-01-08T19:00:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:288"},{"title":"Loading RDD with SQL","text":"%spark\nval sqlRdd = sc.gridSql[DelayRec](\"depDelay > ?\", Seq(15))\nval count = sqlRdd.count()\n","user":"anonymous","dateUpdated":"2018-01-09T08:53:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nsqlRdd: org.insightedge.spark.rdd.InsightEdgeSqlRDD[common.DelayRec] = InsightEdgeSqlRDD[65] at RDD at InsightEdgeAbstractRDD.scala:36\n\ncount: Long = 101704\n"}]},"apps":[],"jobName":"paragraph_1514725957297_-598314384","id":"20170620-093458_2093934595","dateCreated":"2017-12-31T15:12:37+0200","dateStarted":"2018-01-08T19:00:33+0200","dateFinished":"2018-01-08T19:00:34+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289"},{"title":"Create a Machine learning model based on grid data and save the model to the grid","text":"%spark\n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n\n def gen_features(rec: DelayRec): (String, Array[Double]) = {\n      val values = Array(\n        rec.getDepDelay().toDouble,\n        rec.getMonth().toDouble,\n        rec.getDayOfMonth().toDouble,\n        rec.getDayOfWeek().toDouble,\n        rec.getHour().toDouble,\n        rec.getDistance().toDouble\n      )\n      new Tuple2(rec.getId(), values)\n    }\n    \n def parseData(vals: Array[Double]): LabeledPoint = {\n        LabeledPoint(if (vals(0)>=15) 1.0 else 0.0, Vectors.dense(vals.drop(1)))\n    }\n\n// Prepare training set\nval data_2007 = delayRecRdd.map(delayRec => gen_features(delayRec)._2)\nval parsedTrainData = data_2007.map(parseData)\nparsedTrainData.cache\nval scaler = new StandardScaler(withMean = true, withStd = true).fit(parsedTrainData.map(x => x.features))\nval scaledTrainData = parsedTrainData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))\nscaledTrainData.cache\n\n\n\nscaledTrainData.take(3).map(x => (x.label, x.features)).foreach(println)\n\n\n// Build the Logistic Regression model (numIteration should be much higher for accurecy)\nval model_lr = LogisticRegressionWithSGD.train(scaledTrainData, numIterations=20)\n\n\n///ToDo test the model agianst test data, improve using other algo if required and afterward save it to the grid\n\n//Save the model to the grid\nmodel_lr.saveToGrid(sc, \"DelayModel\")\n\n","user":"anonymous","dateUpdated":"2018-01-10T11:43:32+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","title":true,"lineNumbers":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.mllib.feature.StandardScaler\n\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n\ngen_features: (rec: common.DelayRec)(String, Array[Double])\n\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\n\ndata_2007: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[17] at map at <console>:51\n\nparsedTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[18] at map at <console>:53\n\nres18: parsedTrainData.type = MapPartitionsRDD[18] at map at <console>:53\n\nscaler: org.apache.spark.mllib.feature.StandardScalerModel = org.apache.spark.mllib.feature.StandardScalerModel@cc25069\n\nscaledTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[21] at map at <console>:57\n\nres19: scaledTrainData.type = MapPartitionsRDD[21] at map at <console>:57\n(1.0,[1.6159597680040285,0.8276590955608392,1.5332546905137017,1.9038158435893109,0.37432250022338615])\r\n(1.0,[1.6159597680040285,-0.08215598672103099,1.0323284212987425,-0.5197364738037439,0.37432250022338615])\r\n(0.0,[1.6159597680040285,1.6237472925574756,1.5332546905137017,-0.0790905979140975,-0.14002192457031928])\r\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nmodel_lr: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 5, numClasses = 2, threshold = 0.5\n"}]},"apps":[],"jobName":"paragraph_1515055657986_2036408836","id":"20180104-104737_795828311","dateCreated":"2018-01-04T10:47:37+0200","dateStarted":"2018-01-10T11:43:32+0200","dateFinished":"2018-01-10T11:44:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:290"},{"title":"Simulate new data and save it to the grid","text":"%spark\nimport common.DelayRec\nimport org.insightedge.spark.implicits.all._\n\n\nval data = sc.textFile(\"C:/gs/12.3/2008.csv\").map(line => line.split(\",\"))\n.filter(rec => rec(4).toString() != \"NA\")\n.filter(rec => rec(0) !=\"Year\")\n.filter(rec => rec(21).toString() != \"1\")\n.filter(rec => rec(16).toString() == \"ORD\")\n.filter(rec => rec(1).toString() == \"1\")\n.filter(rec => rec(2).toString() == \"3\").map(rec => new DelayRec( rec(9).toString(),rec(0).toInt,rec(1).toInt,rec(2).toInt,rec(3).toInt,rec(5).toInt,rec(15).toInt,rec(16).toString(),rec(18).toDouble))\n   \n//see how the data looks like    \ndata.take(5).map(x => x.toString()).foreach(println)\n\n//save the data to the grid (routing is flight number, id is flight+date)\ndata.saveToGrid()","user":"anonymous","dateUpdated":"2018-01-10T11:43:57+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport common.DelayRec\n\nimport org.insightedge.spark.implicits.all._\n\ndata: org.apache.spark.rdd.RDD[common.DelayRec] = MapPartitionsRDD[74] at map at <console>:57\ncommon.DelayRec@2a27a288\r\ncommon.DelayRec@22798149\r\ncommon.DelayRec@4b492b46\r\ncommon.DelayRec@2e8db5f7\r\ncommon.DelayRec@3929614c\r\n"}]},"apps":[],"jobName":"paragraph_1515409702534_1926912201","id":"20180108-130822_1347993732","dateCreated":"2018-01-08T13:08:22+0200","dateStarted":"2018-01-10T11:43:57+0200","dateFinished":"2018-01-10T11:44:26+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"title":"Predict according to model, save delay objects to grid","text":"%spark\nimport common.FlightDelayed\nimport org.insightedge.spark.implicits.all._\nimport org.apache.spark.mllib.classification.LogisticRegressionModel\n\n//load the model from the grid\nval model = sc.loadMLInstance[LogisticRegressionModel](\"DelayModel\").get\n\n//Load new data(2008) from the grid\nval newdata = sc.gridSql[DelayRec](\"year = ?\", Seq(2008))\nnewdata.count\n\n// Prepare the new data\n\ndef gen_features(rec: DelayRec): (String, Array[Double]) = {\n      val values = Array(\n        rec.getDepDelay().toDouble,\n        rec.getMonth().toDouble,\n        rec.getDayOfMonth().toDouble,\n        rec.getDayOfWeek().toDouble,\n        rec.getHour().toDouble,\n        rec.getDistance().toDouble\n      )\n      new Tuple2(rec.getId(), values)\n    }\n    \n def parseData(vals: Array[Double]): LabeledPoint = {\n        LabeledPoint(if (vals(0)>=15) 1.0 else 0.0, Vectors.dense(vals.drop(1)))\n    }\n\n\n\nval parsedNewData = newdata.map(delayRec => gen_features(delayRec)._2).map(parseData)\nparsedNewData.cache\nval scaledNewData = parsedNewData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))\nscaledNewData.cache\n\n//predict\nval preds_lr = scaledNewData.map(point => model.predict(point.features)) \n  \n\npreds_lr.count()\n\n//zip together data and prediction filter only delayed predictions (prediction 1.0)\nval zipped = newdata.zip(preds_lr).filter(rec => rec._2 == 1.0)\n\n \n//Report to the grid on flight delay predictions\nval predictions = zipped.map { rec => new FlightDelayed(rec._1.getId(), true)}\npredictions.saveToGrid()\n\n","user":"anonymous","dateUpdated":"2018-01-10T11:44:32+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport common.FlightDelayed\n\nimport org.insightedge.spark.implicits.all._\n\nimport org.apache.spark.mllib.classification.LogisticRegressionModel\n\nmodel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 5, numClasses = 2, threshold = 0.5\n\nnewdata: org.insightedge.spark.rdd.InsightEdgeSqlRDD[common.DelayRec] = InsightEdgeSqlRDD[75] at RDD at InsightEdgeAbstractRDD.scala:36\n\nres45: Long = 952\n\ngen_features: (rec: common.DelayRec)(String, Array[Double])\n\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\n\nparsedNewData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[77] at map at <console>:63\n\nres53: parsedNewData.type = MapPartitionsRDD[77] at map at <console>:63\n\nscaledNewData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[78] at map at <console>:74\n\nres54: scaledNewData.type = MapPartitionsRDD[78] at map at <console>:74\n\npreds_lr: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[79] at map at <console>:80\n\nres59: Long = 952\n\nzipped: org.apache.spark.rdd.RDD[(common.DelayRec, Double)] = MapPartitionsRDD[81] at filter at <console>:82\n\npredictions: org.apache.spark.rdd.RDD[common.FlightDelayed] = MapPartitionsRDD[82] at map at <console>:85\n"}]},"apps":[],"jobName":"paragraph_1514964159273_381593931","id":"20180103-092239_1051986871","dateCreated":"2018-01-03T09:22:39+0200","dateStarted":"2018-01-10T11:44:32+0200","dateFinished":"2018-01-10T11:44:47+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:292"},{"text":"%md\n## Congratulations, it's done.\n##### You can create your own notebook in 'Notebook' menu. Good luck!\n","dateUpdated":"2017-12-31T15:12:37+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Congratulations, it&rsquo;s done.</h2>\n<h5>You can create your own notebook in &lsquo;Notebook&rsquo; menu. Good luck!</h5>\n</div>"}]},"apps":[],"jobName":"paragraph_1514725957300_-599468630","id":"20170620-093845_67106827","dateCreated":"2017-12-31T15:12:37+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:293"}],"name":"Flight Delay Prediction example","id":"2D1YK5ZVG","angularObjects":{"2D5CBAWGP:shared_process":[],"2D2Q167PW:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}