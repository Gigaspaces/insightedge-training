{"paragraphs":[{"text":"%md\n\n## Loading data from CSV file\n\nCSV stands for commma-separated values. This is a common text file format in which each line represents a single record, and commas separate each field\nwithin a record.\n","user":"anonymous","dateUpdated":"2018-11-27T16:48:46+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Loading data from CSV file</h2>\n<p>CSV stands for commma-separated values. This is a common text file format in which each line represents a single record, and commas separate each field\n<br  />within a record.</p>\n"}]},"apps":[],"jobName":"paragraph_1543330126216_723738887","id":"20180310-232008_2084436888","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12109"},{"text":"%spark\n\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n\nval dataSchema = new StructType(Array(\n    new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n    new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n    new StructField(\"count\", LongType, false)\n))\n\nval dfFromCsvFile = spark.read.format(?).option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(dataSchema).load(\"my path/2010-summary.csv\")\n    \ndfFromCsvFile.show(?)\n","user":"anonymous","dateUpdated":"2018-11-27T16:58:01+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":false,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\ndataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\ndfFromCsvFile: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1543330126217_27683066","id":"20180310-232058_1854265386","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12110"},{"text":"%md\n\n## Saving DataFrame to a CSV file\n\nWe can take our CSV file and write it out as a TSV file (with tabs separating data fields)\n\nWhen we list the destination directory, we will see that my-tsv-file is actually a folder with numerous files within it. This actually reflects the number of paritions in our DataFrame at that time. If we were to repartition our data before then, we would end up with a different number of files.\n","user":"anonymous","dateUpdated":"2018-11-27T16:48:46+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Saving DataFrame to a CSV file</h2>\n<p>We can take our CSV file and write it out as a TSV file (with tabs separating data fields)</p>\n<p>When we list the destination directory, we will see that my-tsv-file is actually a folder with numerous files within it. This actually reflects the number of paritions in our DataFrame at that time. If we were to repartition our data before then, we would end up with a different number of files.</p>\n"}]},"apps":[],"jobName":"paragraph_1543330126218_-303050969","id":"20180310-233015_1064163149","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12111"},{"text":"%spark\n\ndfFromCsvFile.write.format(?).mode(?).option(\"sep\", \"\\t\").save(\"my path/output/my-tsv-file.tsv\")","user":"anonymous","dateUpdated":"2018-11-27T16:58:34+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1543330126218_110579181","id":"20180310-232850_120441597","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12112"},{"text":"%md\n\n## Using Parquet files with Spark\n\nParquet is an interoperable columnar storage format. It focusses on space efficiency and query efficiency. Parquet's origin is based on Google's Dremel and was developed by Twitter and Cloudera.\n\nParquet is now an Apache project - [Apache Parquet](https://parquet.apache.org).\n\nSpark can load data from Parquet files and save data as Parquet files.\n\n","user":"anonymous","dateUpdated":"2018-11-27T16:48:46+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Using Parquet files with Spark</h2>\n<p>Parquet is an interoperable columnar storage format. It focusses on space efficiency and query efficiency. Parquet&rsquo;s origin is based on Google&rsquo;s Dremel and was developed by Twitter and Cloudera.</p>\n<p>Parquet is now an Apache project - <a href=\"https://parquet.apache.org\">Apache Parquet</a>.</p>\n<p>Spark can load data from Parquet files and save data as Parquet files.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1543330126218_777073715","id":"20160822-165125_1131574017","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12113"},{"title":"Loading data from Parquet file","text":"%spark\n\nval flightData = spark.?.format(?).load(\"my path/2010-summary.parquet\")\n\nflightData.show(?)\n","user":"anonymous","dateUpdated":"2018-11-27T16:59:03+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"flightData: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1543330126219_1479370903","id":"20160823-111517_1662906000","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12114"},{"title":"Saving data to Parquet file","text":"%spark\n\ndfFromCsvFile.write.format(?).mode(\"overwrite\").save(\"my path/output/2010-summary.parquet\")\n","user":"anonymous","dateUpdated":"2018-11-27T16:59:25+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1543330126220_-1997682707","id":"20160823-112553_610561150","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12115"},{"text":"%md\n\n## Using JSON files with Spark\n\nSpark can read data from JSON files.\n\nThere are some catches when working with this kind of data that are worth considering before we jump in. \n\nIn Spark, when we refer to JSON files, we refer to _line-delimited_ JSON files. This contrasts with files that have a large JSON object or array per file.\n\nThe _line-delimited_ versus _multiline_ trade-off is controlled by a single option: **multiLine**. \n\nWhen you set this option to true, you can read an entire file as one json object and Spark will go through the work of parsing that into a\nDataFrame. \n\nLine-delimited JSON is actually a much more stable format because it allows you to append to a file with a new record (rather than\nhaving to read in an entire file and then write it out), which is what we recommend that you use. \n\nAnother key reason for the popularity of line-delimited JSON is because JSON objects have structure, and JavaScript (on which JSON is based) has at least basic types. This makes it easier to work with because Spark can make more assumptions on our behalf about the data.\n\n","user":"anonymous","dateUpdated":"2018-11-27T16:48:46+0200","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Using JSON files with Spark</h2>\n<p>Spark can read data from JSON files.</p>\n<p>There are some catches when working with this kind of data that are worth considering before we jump in.</p>\n<p>In Spark, when we refer to JSON files, we refer to <em>line-delimited</em> JSON files. This contrasts with files that have a large JSON object or array per file.</p>\n<p>The <em>line-delimited</em> versus <em>multiline</em> trade-off is controlled by a single option: <strong>multiLine</strong>.</p>\n<p>When you set this option to true, you can read an entire file as one json object and Spark will go through the work of parsing that into a\n<br  />DataFrame.</p>\n<p>Line-delimited JSON is actually a much more stable format because it allows you to append to a file with a new record (rather than\n<br  />having to read in an entire file and then write it out), which is what we recommend that you use.</p>\n<p>Another key reason for the popularity of line-delimited JSON is because JSON objects have structure, and JavaScript (on which JSON is based) has at least basic types. This makes it easier to work with because Spark can make more assumptions on our behalf about the data.</p>\n"}]},"apps":[],"jobName":"paragraph_1543330126221_-1216698077","id":"20160823-120136_710583192","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12116"},{"title":"Loading data from JSON","text":"%spark\n\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n\nval dataSchema = new StructType(Array(\n    new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n    new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, ?),\n    new StructField(\"count\", LongType, false)\n))\n\nval dfFromJson = spark.read.format(?).option(\"mode\", \"FAILFAST\").load(\"my path/2010-summary.json\")\n\ndfFromJson.show(?)\n\n// print the schema\ndfFromJson.?\n","user":"anonymous","dateUpdated":"2018-11-27T17:01:14+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\ndataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\ndfFromJson: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\nroot\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1543330126222_1788311639","id":"20160823-120614_1736879537","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12117"},{"title":"Saving data as JSON file","text":"%spark\n\ndfFromJson.write.format(?).mode(\"overwrite\").save(\"my path/output/my-jsonfile.json\")\n    \n","user":"anonymous","dateUpdated":"2018-11-27T17:01:37+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1543330126223_859064177","id":"20160823-120800_314999582","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12118"},{"title":"Reading data from JSON file whose schema is known","text":"%spark\n\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n\nval dataSchema = new StructType(Array(\n    new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n    new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n    new StructField(\"count\", LongType, true)\n))\n\nval dfFromJson = spark.read.format(?).option(\"mode\", \"FAILFAST\").schema(?).load(\"my path/2010-summary.json\")\n\ndfFromJson.show(?)\n","user":"anonymous","dateUpdated":"2018-11-27T17:02:24+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\ndataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\ndfFromJson: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1543330126224_1604045244","id":"20160823-122854_1466425181","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12119"},{"text":"","user":"anonymous","dateUpdated":"2018-11-27T16:48:46+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1543330126225_-766519313","id":"20160825-005916_631693256","dateCreated":"2018-11-27T16:48:46+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12120"}],"name":"Lab 1.2 exercise - DataFrame - Loading and Saving Data","id":"2DYDUR3QK","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}