{"paragraphs":[{"title":"Getting information about a Dataframe","text":"%spark\n\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n\nval dataSchema = new StructType(Array(\n    new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n    new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n    new StructField(\"count\", LongType, false)\n))\n\nval flightDf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(dataSchema).load(\"my path/2010-summary.csv\")\n\n// cache the data frame     \n...\n\n// print the DataFrame columns\n...\n\n// prints the DataFrame physical plan\n...\n\n// print the DataFram column names and their data types as an array\n...\n\n// print the DataFrame schema\n...\n\n// print 5 rows\n...\n","user":"anonymous","dateUpdated":"2018-12-06T13:35:23+0200","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\ndataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\nflightDf: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres38: flightDf.type = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\nres39: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)\n== Physical Plan ==\nInMemoryTableScan [DEST_COUNTRY_NAME#346, ORIGIN_COUNTRY_NAME#347, count#348L]\n   +- InMemoryRelation [DEST_COUNTRY_NAME#346, ORIGIN_COUNTRY_NAME#347, count#348L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- *(1) FileScan csv [DEST_COUNTRY_NAME#346,ORIGIN_COUNTRY_NAME#347,count#348L] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/yuval/Workspace/insightedge-training/Core-IE-And-Spark-APIs/labs/Da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\nres41: Array[(String, String)] = Array((DEST_COUNTRY_NAME,StringType), (ORIGIN_COUNTRY_NAME,StringType), (count,LongType))\nroot\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1543925886108_1263629274","id":"20160822-145305_1775455394","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1931"},{"text":"%md\n\n## Language-integrated query methods of the DataFrame class\n\nWe will create some dataframes programmatically to illustrate the language integrated query methods.","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"tableHide":true,"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Language-integrated query methods of the DataFrame class</h2>\n<p>We will create some dataframes programmatically to illustrate the language integrated query methods.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1543925886120_-2100732953","id":"20160823-174226_141584309","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1932"},{"text":"%spark\n\n// create the Customer DataFrame, the product DataFrame and the Sales DataFrame \n\ncase class Customer(cId: Long, name: String, age: Int, gender: String)\nval customers = List(\n    Customer(1, \"James\", 21, \"M\"),\n    Customer(2, \"Liz\", 25, \"F\"),\n    Customer(3, \"John\", 31, \"M\"),\n    Customer(4, \"Jennifer\", 45, \"F\"),\n    Customer(5, \"Robert\", 41, \"M\"),\n    Customer(6, \"Sandra\", 45, \"F\")\n)\nval customerDF = sc.parallelize(customers).toDF()\n\n// \n\ncase class Product(pId: Long, name: String, price: Double, cost: Double)\n\nval products = List(\n    Product(1, \"Dell\", 600, 400),\n    Product(2, \"Kindle\", 100, 40),\n    Product(3, \"iPad\", 600, 500),\n    Product(4, \"Galaxy\", 600, 400),\n    Product(5, \"MacBook\", 1200, 900),\n    Product(6, \"iPhone\", 500, 400)\n)\nval productDF = sc.parallelize(products).toDF()\n\ncase class SalesSummary(date: String, product: String, country: String, revenue: Double)\n\nval sales = List(SalesSummary(\"01/01/2015\", \"Chromebook\", \"USA\", 40000),\nSalesSummary(\"01/02/2015\", \"Chromebook\", \"USA\", 30000),\nSalesSummary(\"01/01/2015\", \"Chromebook\", \"India\", 10000),\nSalesSummary(\"01/02/2015\", \"Chromebook\", \"India\", 5000),\nSalesSummary(\"01/01/2015\", \"Macbook\", \"USA\", 20000),\nSalesSummary(\"01/02/2015\", \"Macbook\", \"USA\", 10000),\nSalesSummary(\"01/01/2015\", \"Macbook\", \"India\", 9000),\nSalesSummary(\"01/02/2015\", \"Macbook\", \"India\", 6000))\n\nval salesDF = sc.parallelize(sales).toDF()\n\n// hint: nothing to do beside running the paragraph.","user":"anonymous","dateUpdated":"2018-12-06T13:46:19+0200","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Customer\ncustomers: List[Customer] = List(Customer(1,James,21,M), Customer(2,Liz,25,F), Customer(3,John,31,M), Customer(4,Jennifer,45,F), Customer(5,Robert,41,M), Customer(6,Sandra,45,F))\ncustomerDF: org.apache.spark.sql.DataFrame = [cId: bigint, name: string ... 2 more fields]\ndefined class Product\nproducts: List[Product] = List(Product(1,Dell,600.0,400.0), Product(2,Kindle,100.0,40.0), Product(3,iPad,600.0,500.0), Product(4,Galaxy,600.0,400.0), Product(5,MacBook,1200.0,900.0), Product(6,iPhone,500.0,400.0))\nproductDF: org.apache.spark.sql.DataFrame = [pId: bigint, name: string ... 2 more fields]\ndefined class SalesSummary\nsales: List[SalesSummary] = List(SalesSummary(01/01/2015,Chromebook,USA,40000.0), SalesSummary(01/02/2015,Chromebook,USA,30000.0), SalesSummary(01/01/2015,Chromebook,India,10000.0), SalesSummary(01/02/2015,Chromebook,India,5000.0), SalesSummary(01/01/2015,Macbook,USA,20000.0), SalesSummary(01/02/2015,Macbook,USA,10000.0), SalesSummary(01/01/2015,Macbook,India,9000.0), SalesSummary(01/02/2015,Macbook,India,6000.0))\nsalesDF: org.apache.spark.sql.DataFrame = [date: string, product: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1543925886122_1571298647","id":"20160823-175111_1341992873","dateCreated":"2018-12-04T14:18:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1933","dateFinished":"2018-12-06T13:43:38+0200","dateStarted":"2018-12-06T13:43:04+0200"},{"text":"%md\n\n## agg\n\nThe **agg** method performs specified aggregations on one or more columns in the source DataFrame and\nreturns the result as a new DataFrame.","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>agg</h2>\n<p>The <strong>agg</strong> method performs specified aggregations on one or more columns in the source DataFrame and<br/>returns the result as a new DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1543925886123_-769027559","id":"20160823-175710_728578499","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1934"},{"text":"%spark\n\n// print the following information from products DataFrame:\n// 1. the highest price.\n// 2. the lowest price.\n// 3. total namber of products.","user":"anonymous","dateUpdated":"2018-12-06T13:51:14+0200","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aggregates: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1543925886126_526894568","id":"20160823-180618_860695558","dateCreated":"2018-12-04T14:18:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1935","dateFinished":"2018-12-06T13:46:45+0200","dateStarted":"2018-12-06T13:46:41+0200","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4041/jobs/job?id=0"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n## apply\n\nThe **apply** method takes the name of a column as an argument and returns the specified column in the\nsource DataFrame as an instance of the **Column** class. The **Column** class provides operators for manipulating a\ncolumn in a DataFrame.\n","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>apply</h2>\n<p>The <strong>apply</strong> method takes the name of a column as an argument and returns the specified column in the<br/>source DataFrame as an instance of the <strong>Column</strong> class. The <strong>Column</strong> class provides operators for manipulating a<br/>column in a DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1543925886128_310614814","id":"20160823-180643_1652513120","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1936"},{"text":"%spark\n\n// create an instance of Column class and initialized it with 10% of the original price","user":"anonymous","dateUpdated":"2018-12-06T14:01:41+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"priceColumn: org.apache.spark.sql.Column = price\ndiscountedPriceColumn: org.apache.spark.sql.Column = (price * 0.1)\n"}]},"apps":[],"jobName":"paragraph_1543925886131_-792517989","id":"20160823-180831_1959446161","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1937"},{"text":"%md\n\nScala provides syntactic sugar that allows you to use productDF(\"price\") instead of productDF.apply(\"price\"). \n\nIt automatically converts productDF(\"price\") to productDF.apply(\"price\"). \n\nSo the preceding code can be rewritten as follows:","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Scala provides syntactic sugar that allows you to use productDF(&ldquo;price&rdquo;) instead of productDF.apply(&ldquo;price&rdquo;).</p>\n<p>It automatically converts productDF(&ldquo;price&rdquo;) to productDF.apply(&ldquo;price&rdquo;).</p>\n<p>So the preceding code can be rewritten as follows:</p>\n"}]},"apps":[],"jobName":"paragraph_1543925886133_-614623087","id":"20160823-181039_165411325","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1938"},{"text":"%spark\n\n// rewrite the previous paragraph with the above suggaring.","user":"anonymous","dateUpdated":"2018-12-06T14:03:26+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"priceColumn: org.apache.spark.sql.Column = price\ndiscountedPriceColumn: org.apache.spark.sql.Column = (price * 0.5)\n"}]},"apps":[],"jobName":"paragraph_1543925886134_1245322027","id":"20160823-181124_19808600","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1939"},{"text":"%md\n\n## Notes on the **Column** class\n\nAn instance of the **Column** class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library.\n\nIf a method or function expects an instance of the Column class as an argument, we can use the **$\"...\"** notation to select a column in a DataFrame.\n\nFor example, the following three aggregates are equivalent:\n\n","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Notes on the <strong>Column</strong> class</h2>\n<p>An instance of the <strong>Column</strong> class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library.</p>\n<p>If a method or function expects an instance of the Column class as an argument, we can use the <strong>$&ldquo;&hellip;&ldquo;</strong> notation to select a column in a DataFrame.</p>\n<p>For example, the following three aggregates are equivalent:</p>\n"}]},"apps":[],"jobName":"paragraph_1543925886136_-280067850","id":"20160823-181235_559377109","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1940"},{"text":"%spark\n\n// the long way\nval aggregates1 = productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")), count(productDF(\"name\")))\n\naggregates1.show\n\n// remove the DataFram prefix\nval aggregates2 = productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\n\naggregates2.show\n\n// using $ to select the column in a DataFrame\nval aggregates3 = productDF.agg(max($\"price\"), min($\"price\"), count($\"name\"))\n\naggregates3.show\n\n// hint: nothing to do beside running the paragraph \n","user":"anonymous","dateUpdated":"2018-12-06T14:35:22+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aggregates1: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\naggregates2: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\naggregates3: org.apache.spark.sql.DataFrame = [max(price): double, min(price): double ... 1 more field]\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     100.0|          6|\n+----------+----------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1543925886139_2106111506","id":"20160823-181903_1519059300","dateCreated":"2018-12-04T14:18:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1941","dateFinished":"2018-12-06T14:35:13+0200","dateStarted":"2018-12-06T14:35:09+0200","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4041/jobs/job?id=1","http://192.168.9.185:4041/jobs/job?id=2","http://192.168.9.185:4041/jobs/job?id=3"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n### **select**\n\nThe *select* method returns a DataFrame containing only the specified columns from the source DataFrame.\n","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"tableHide":true,"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>select</strong></h3>\n<p>The <em>select</em> method returns a DataFrame containing only the specified columns from the source DataFrame.</p>\n"}]},"apps":[],"jobName":"paragraph_1543925886146_-249267443","id":"20160825-004227_2034053977","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1942"},{"text":"%spark\n\n// pring the customers name and age","user":"anonymous","dateUpdated":"2018-12-06T14:37:48+0200","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"namesAgeDF: org.apache.spark.sql.DataFrame = [name: string, age: int]\n+--------+---+\n|    name|age|\n+--------+---+\n|   James| 21|\n|     Liz| 25|\n|    John| 31|\n|Jennifer| 45|\n|  Robert| 41|\n|  Sandra| 45|\n+--------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1543925886147_-660524672","id":"20160825-004303_1682213357","dateCreated":"2018-12-04T14:18:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1943","dateFinished":"2018-12-06T14:36:32+0200","dateStarted":"2018-12-06T14:36:31+0200","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4041/jobs/job?id=4","http://192.168.9.185:4041/jobs/job?id=5"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n### **filter**\n\nThe filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>filter</strong></h3>\n<p>The filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.</p>\n"}]},"apps":[],"jobName":"paragraph_1543925886150_-1978256701","id":"20160823-182045_153856142","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1944"},{"text":"%spark\n\n// print all customers information older than 20 (the long way)\nval filteredDF3 = customerDF.filter(customerDF(\"age\") > 20)\n\nfilteredDF3.show\n// print all customers information older than 25 (omit the DataFrame if you can)\n\n// print all customers information older than 40 (use the \"$\" notation)","user":"anonymous","dateUpdated":"2018-12-06T14:53:53+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filteredDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cId: bigint, name: string ... 2 more fields]\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\nfilteredDF2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cId: bigint, name: string ... 2 more fields]\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\nfilteredDF3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cId: bigint, name: string ... 2 more fields]\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  2|     Liz| 25|     F|\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\n"}]},"apps":[],"jobName":"paragraph_1543925886152_-441736669","id":"20160825-002529_441878203","dateCreated":"2018-12-04T14:18:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1945","dateFinished":"2018-12-06T14:38:52+0200","dateStarted":"2018-12-06T14:38:49+0200","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4041/jobs/job?id=6","http://192.168.9.185:4041/jobs/job?id=7","http://192.168.9.185:4041/jobs/job?id=8","http://192.168.9.185:4041/jobs/job?id=9","http://192.168.9.185:4041/jobs/job?id=10","http://192.168.9.185:4041/jobs/job?id=11"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n### **groupBy**\n\nThe *groupBy* method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.\n","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>groupBy</strong></h3>\n<p>The <em>groupBy</em> method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.</p>\n"}]},"apps":[],"jobName":"paragraph_1543925886154_1881139981","id":"20160825-003138_1418319608","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1946"},{"text":"%spark\n\n// print the number females and males.\nval countByGender = customerDF.groupBy(\"gender\").count\n\ncountByGender.show\n\n// pring the total revenue per product","user":"anonymous","dateUpdated":"2018-12-06T14:51:37+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1543925886157_975511896","id":"20160825-002634_332193277","dateCreated":"2018-12-04T14:18:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1947","dateFinished":"2018-12-06T14:49:18+0200","dateStarted":"2018-12-06T14:49:15+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"countByGender: org.apache.spark.sql.DataFrame = [gender: string, count: bigint]\n+------+-----+\n|gender|count|\n+------+-----+\n|     F|    3|\n|     M|    3|\n+------+-----+\n\nrevenueByProductDF: org.apache.spark.sql.DataFrame = [product: string, sum(revenue): double]\n+----------+------------+\n|   product|sum(revenue)|\n+----------+------------+\n|   Macbook|     45000.0|\n|Chromebook|     85000.0|\n+----------+------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4041/jobs/job?id=27","http://192.168.9.185:4041/jobs/job?id=28","http://192.168.9.185:4041/jobs/job?id=29","http://192.168.9.185:4041/jobs/job?id=30","http://192.168.9.185:4041/jobs/job?id=31","http://192.168.9.185:4041/jobs/job?id=32","http://192.168.9.185:4041/jobs/job?id=33","http://192.168.9.185:4041/jobs/job?id=34","http://192.168.9.185:4041/jobs/job?id=35","http://192.168.9.185:4041/jobs/job?id=36"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n### **orderBy**\n\nThe *orderBy* method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><strong>orderBy</strong></h3>\n<p>The <em>orderBy</em> method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.</p>\n"}]},"apps":[],"jobName":"paragraph_1543925886158_-816499227","id":"20160825-003347_1250302752","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1948"},{"text":"%spark\n\n// print all customers information sorted by name","user":"anonymous","dateUpdated":"2018-12-06T14:54:13+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1543925886160_1877997638","id":"20160825-004007_244050346","dateCreated":"2018-12-04T14:18:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1949","dateFinished":"2018-12-06T14:52:12+0200","dateStarted":"2018-12-06T14:52:11+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sortedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cId: bigint, name: string ... 2 more fields]\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  4|Jennifer| 45|     F|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.9.185:4041/jobs/job?id=37"],"interpreterSettingId":"spark"}}},{"text":"","user":"anonymous","dateUpdated":"2018-12-04T14:18:06+0200","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1543925886162_391292717","id":"20160825-005916_631693256","dateCreated":"2018-12-04T14:18:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1950"}],"name":"Exercise - DataFrame - Operations","id":"2DX8SXBUB","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}